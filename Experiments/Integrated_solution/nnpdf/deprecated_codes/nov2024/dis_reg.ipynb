{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Keras backend\n"
     ]
    }
   ],
   "source": [
    "from validphys.api import API\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from validphys.fkparser import load_fktable\n",
    "from n3fit.layers.rotations import FlavourToEvolution\n",
    "from collections import defaultdict\n",
    "\n",
    "from typing import List, Dict\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.typing import ArrayLike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1341351341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequential_model(outputs=1, \n",
    "                   input_layer=None, \n",
    "                   nlayers=2, \n",
    "                   units=[100,100],\n",
    "                   seed=seed,\n",
    "                   **kwargs):\n",
    "  \"\"\"\n",
    "  Create a tensorflow sequential model where all intermediate layers have the same size\n",
    "  This function accepts an already constructed layer as the input.\n",
    "\n",
    "  All hidden layers will have the same number of nodes for simplicity\n",
    "\n",
    "  Arguments:\n",
    "      outputs: int (default=1)\n",
    "          number of output nodes (how many flavours are we training)\n",
    "      input_layer: KerasTensor (default=None)\n",
    "          if given, sets the input layer of the sequential model\n",
    "      nlayers: int\n",
    "          number of hidden layers of the network\n",
    "      units: int\n",
    "          number of nodes of every hidden layer in the network\n",
    "      activation: str\n",
    "          activation function to be used by the hidden layers (ex: 'tanh', 'sigmoid', 'linear')\n",
    "  \"\"\"\n",
    "  if len(units) != nlayers:\n",
    "      raise Exception(\"The length of units must match the number of layers.\")\n",
    "  \n",
    "  if kwargs.get('kernel_initializer'):\n",
    "      kernel_initializer = kwargs['kernel_initializer']\n",
    "  else:\n",
    "      kernel_initializer = tf.keras.initializers.HeNormal\n",
    "\n",
    "  if kwargs.get('activation_list'):\n",
    "      activation_list = kwargs['activation_list']\n",
    "      if len(units) != len(activation_list):\n",
    "          raise Exception(\"The length of the activation list must match the number of layers.\")\n",
    "  else:\n",
    "      activation_list = ['tanh' for _ in range(nlayers)]\n",
    "\n",
    "  if kwargs.get('output_func'):\n",
    "      output_func = kwargs['output_func']\n",
    "  else:\n",
    "      output_func = 'linear'\n",
    "  \n",
    "  if kwargs.get('name'):\n",
    "      name = kwargs['name']\n",
    "  else:\n",
    "      name = 'pdf'\n",
    "  \n",
    "  model = tf.keras.models.Sequential(name=name)\n",
    "  if input_layer is not None:\n",
    "      model.add(input_layer)\n",
    "  for layer in range(nlayers):\n",
    "      model.add(tf.keras.layers.Dense(units[layer], \n",
    "                                      activation=activation_list[layer],\n",
    "                                      kernel_initializer=kernel_initializer(seed=seed - layer),\n",
    "                                      ),\n",
    "      )\n",
    "  model.add(tf.keras.layers.Dense(outputs, \n",
    "                                  activation=output_func, \n",
    "                                  kernel_initializer=kernel_initializer(seed=seed - nlayers)\n",
    "                                  ))\n",
    "\n",
    "  return model\n",
    "\n",
    "def compute_ntk(model, input):\n",
    "  # Record operations for gradient computation\n",
    "  batch_size = input.size\n",
    "  n_outputs = model.layers[-1].units\n",
    "  x = tf.convert_to_tensor(input)\n",
    "  x = tf.reshape(x, shape=(-1,1))\n",
    "  with tf.GradientTape(persistent=True) as tape:\n",
    "      tape.watch(x)\n",
    "      # Forward pass\n",
    "      predictions = model(x)\n",
    "\n",
    "  jacobian = tape.jacobian(predictions, model.trainable_variables)  \n",
    "\n",
    "  ntk = tf.zeros((n_outputs, batch_size, n_outputs, batch_size))\n",
    "  for jac in jacobian:\n",
    "      jac_concat = tf.reshape(jac, (jac.shape[1], jac.shape[0], np.prod(jac.shape[2:]))) \n",
    "      ntk += tf.einsum('iap,jbp->iajb', jac_concat, jac_concat)  \n",
    "  return ntk\n",
    "\n",
    "\n",
    "def compute_mr(A : ArrayLike, \n",
    "               C : ArrayLike,\n",
    "               Gamma : ArrayLike = None, \n",
    "               eta : float = None):\n",
    "    '''Compute the tensor M that defines the inverse problem.\n",
    "       The tensor can be regularised by means of the tensor gamma.\n",
    "       THe magnitude if the regularization is specified by the scalar eta.\n",
    "    '''\n",
    "    M = np.tensordot(C, A, axes=[[1],[0]])\n",
    "    M = np.tensordot(A, M, axes=[[0],[0]])\n",
    "    R = np.zeros_like(M)\n",
    "    if Gamma is not None and eta is not None:\n",
    "        R = eta * np.tensordot(Gamma, Gamma, axes=[[0],[0]])\n",
    "    return M + R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_inputs = [\n",
    "  #{'dataset': 'NMC_NC_NOTFIXED_DW_EM-F2', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'NMC_NC_NOTFIXED_P_EM-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'SLAC_NC_NOTFIXED_P_DW_EM-F2', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'SLAC_NC_NOTFIXED_D_DW_EM-F2', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'BCDMS_NC_NOTFIXED_P_DW_EM-F2', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'BCDMS_NC_NOTFIXED_D_DW_EM-F2', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'CHORUS_CC_NOTFIXED_PB_DW_NU-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'CHORUS_CC_NOTFIXED_PB_DW_NB-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'NUTEV_CC_NOTFIXED_FE_DW_NU-SIGMARED', 'cfac': ['MAS'], 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'NUTEV_CC_NOTFIXED_FE_DW_NB-SIGMARED', 'cfac': ['MAS'], 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'HERA_NC_318GEV_EM-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'HERA_NC_225GEV_EP-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'HERA_NC_251GEV_EP-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'HERA_NC_300GEV_EP-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'HERA_NC_318GEV_EP-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'HERA_CC_318GEV_EM-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'HERA_CC_318GEV_EP-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'HERA_NC_318GEV_EAVG_CHARM-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'HERA_NC_318GEV_EAVG_BOTTOM-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dict = dict(\n",
    "    dataset_inputs=dataset_inputs,\n",
    "    metadata_group=\"nnpdf31_process\",\n",
    "    use_cuts='internal',\n",
    "    datacuts={'t0pdfset': '240701-02-rs-nnpdf40-baseline', 'q2min': 3.49, 'w2min': 12.5},\n",
    "    theoryid=40000000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_data = API.procs_data(**common_dict)\n",
    "groups_index = API.groups_index(**common_dict)\n",
    "C = API.groups_covmat_no_table(**common_dict)\n",
    "Cinv = np.linalg.inv(C)\n",
    "Cinv = pd.DataFrame(Cinv, index=C.index, columns=C.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fk_table_list = defaultdict(list)\n",
    "x_grid_list = defaultdict(list)\n",
    "central_values = defaultdict(list)\n",
    "Y = defaultdict(list)\n",
    "\n",
    "total_ndata_wc = 0\n",
    "total_grid_size = 0\n",
    "start_grid_by_exp = defaultdict(list)\n",
    "grid_size_by_exp = defaultdict(list)\n",
    "start_proc_by_exp = defaultdict(list)\n",
    "M_list = []\n",
    "\n",
    "for idx_proc, group_proc in enumerate(groups_data):\n",
    "  for idx_exp, exp_set in enumerate(group_proc.datasets):\n",
    "\n",
    "    fkspecs = exp_set.fkspecs\n",
    "    cuts = exp_set.cuts\n",
    "    ndata = exp_set.load_commondata().ndata\n",
    "    fk_table = load_fktable(fkspecs[0])\n",
    "    fk_table_wc = fk_table.with_cuts(cuts)\n",
    "    x_grid = fk_table_wc.xgrid\n",
    "    fk_table_wc_np = fk_table_wc.get_np_fktable()\n",
    "\n",
    "    Y[exp_set.name] = exp_set.load_commondata().with_cuts(cuts).central_values.to_numpy()\n",
    "    fk_table_list[exp_set.name] = fk_table_wc_np\n",
    "    x_grid_list[exp_set.name] = x_grid\n",
    "    start_proc_by_exp[exp_set.name] = total_ndata_wc\n",
    "    start_grid_by_exp[exp_set.name] = total_grid_size\n",
    "    grid_size_by_exp[exp_set.name] = x_grid.shape[0]\n",
    "    total_grid_size += x_grid.shape[0]\n",
    "    total_ndata_wc += ndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "evol_basis_pids = tuple(\n",
    "      [22, 100, 21, 200]\n",
    "      + [200 + n**2 - 1 for n in range(2, 6 + 1)]\n",
    "      + [100 + n**2 - 1 for n in range(2, 6 + 1)]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 100, 21, 200, 203, 208, 215, 224, 235, 103, 108, 115, 124, 135)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evol_basis_pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mgrid\u001b[49m\u001b[38;5;241m.\u001b[39mchannels()):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grid' is not defined"
     ]
    }
   ],
   "source": [
    "for idx, c in enumerate(grid.channels()):\n",
    "    print(f\"{idx}: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "flav_map = {\n",
    "        0: \"g\",\n",
    "        1: \"t3\",\n",
    "        2: \"t8\",\n",
    "        3: \"t15 (c-)\",\n",
    "        4: \"sigma\",\n",
    "        5: \"v3\",\n",
    "        6: \"v8\",\n",
    "        7: \"v15\",\n",
    "        8: \"v\",}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with one single DIS data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fk_table_list.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "expname = 'HERA_NC_225GEV_EP-SIGMARED'\n",
    "fk_exp = fk_table_list[expname]\n",
    "Cinv_exp = Cinv.xs(level='dataset', key=expname).T.xs(level='dataset', key=expname)\n",
    "xgrid_exp = x_grid_list[expname]\n",
    "y_exp = Y[expname]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NNPDF-like model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate NNPDF model\n",
    "nnpdf = generate_sequential_model(outputs=9, nlayers=2, units=[28, 20], seed=seed, name='NNPDF', kernel_initializer=tf.keras.initializers.GlorotNormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NTK in the grid space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the ntk\n",
    "ntk_exp = compute_ntk(nnpdf, xgrid_exp).numpy()\n",
    "ntk = ntk_exp.reshape(ntk_exp.shape[0] * ntk_exp.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the matrix M\n",
    "Cinv_fk = np.tensordot(Cinv_exp, fk_exp, axes=[[1],[0]])\n",
    "M = np.tensordot(fk_exp, Cinv_fk, axes=[[0],[0]])\n",
    "M = M.reshape(M.shape[0] * M.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implicit regularisation of GD\n",
    "eta_gd = 0.001\n",
    "\n",
    "# Implicit regularisation from gradient descent\n",
    "#ntk_M = np.tensordot(ntk_exp, M, axes=[[2,3],[0,1]])\n",
    "#M_ntk_M = np.tensordot(M, ntk_M, axes=[[0,1],[0,1]])\n",
    "#M_ntk_M.reshape(M_ntk_M.shape[0] * M_ntk_M.shape[1], -1)\n",
    "\n",
    "M_gd = eta_gd * ( M.T @ ntk @ M + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity regularisation\n",
    "eta_id = 0.001\n",
    "M_id = eta_id * np.identity(M.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert the matrix\n",
    "Mr = M - M_gd\n",
    "np.linalg.inv(Mr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eta_gd = 0\n",
    "#eta_np_reg = 0.001\n",
    "#\n",
    "## Implicit regularisation from gradient descent\n",
    "#ntk_M = np.tensordot(ntk_exp, M, axes=[[2,3],[0,1]])\n",
    "#M_ntk_M = np.tensordot(M, ntk_M, axes=[[0,1],[0,1]])\n",
    "#M_ntk_M.reshape(M_ntk_M.shape[0] * M_ntk_M.shape[1], -1)\n",
    "#Mr_exp = M - eta_gd * M_ntk_M\n",
    "#\n",
    "## flavour regularisation through identity\n",
    "##Mr_exp_id = np.zeros_like(Mr_exp)\n",
    "##for alpha in range(Mr_exp.shape[0]):\n",
    "##  for beta in range(Mr_exp.shape[2]):\n",
    "##    Id = eta_id * np.identity(M.shape[1])\n",
    "##    Mr_exp_id[alpha, : , beta, :] = Mr_exp[alpha, : , beta, :] + Id\n",
    "#\n",
    "## Regularisation numpy-like\n",
    "#prod = 1\n",
    "#shape = Mr_exp.shape\n",
    "#ind = 2\n",
    "#invshape = shape[ind:] + shape[:ind]\n",
    "#for k in shape[ind:]:\n",
    "#  prod *= k\n",
    "#a = Mr_exp.reshape(prod, -1)\n",
    "#Id = np.identity(prod)\n",
    "#a = a + eta_np_reg * Id\n",
    "#ia = np.linalg.inv(a)\n",
    "#Mr_inv = ia#.reshape(*invshape)\n",
    "#Mr_collapsed = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mr_exp_tensor_inv = np.linalg.tensorinv(Mr_id_reg, ind=2)\n",
    "# Check if it corresponds to the actual inverse\n",
    "rng = np.random.default_rng()\n",
    "b = rng.normal(size=(9, 26))\n",
    "np.allclose(np.tensordot(Mr_exp_tensor_inv, b, axes=[[2,3], [0,1]]), np.linalg.tensorsolve(Mr_id_reg, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compact the tensor $M$ into a matrix and compute the eigenvalues (useful?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_compact = Mr_id_reg.reshape(prod,-1)\n",
    "eigvals, eigvecs = np.linalg.eig(M_compact)\n",
    "for i, val in enumerate(eigvals):\n",
    "  print(f'lambda_{i+1} = {val.real} + i {val.imag}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in range(Mr_id_reg.shape[0]):\n",
    "  for beta in range(Mr_id_reg.shape[2]):\n",
    "    eigvals, eigvecs = np.linalg.eig(Mr_id_reg[alpha,:,beta,:])\n",
    "    #eigvals = eigvals.real\n",
    "    print(f'Eigvalues for flavour combination ({flav_map[alpha]}, {flav_map[beta]})')\n",
    "    print(f\"{np.sort(eigvals)}\")\n",
    "    print('___________________________\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the matrixes for each flavour are invertible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import LinAlgError\n",
    "\n",
    "Mr_exp_inv = np.zeros_like(Mr_exp)\n",
    "for alpha in range(Mr_exp.shape[0]):\n",
    "  for beta in range(Mr_exp.shape[2]):\n",
    "    try:\n",
    "      Mr_exp_inv[alpha, : , beta, :] = np.linalg.inv(Mr_exp_id[alpha,:,beta,:])\n",
    "    except LinAlgError:\n",
    "      print(f\"({alpha}, {beta}) not invertible\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mr_exp_inv = np.linalg.tensorinv(Mr_exp, 2)\n",
    "#delta = np.tensordot(Mr_exp_inv, Mr_exp, axes=[[2,3], [0,1]])\n",
    "#print(delta[4,:,4,:].diagonal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing that the inverse is really what it claims to be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing $H = \\Theta M_R$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse ntk\n",
    "prod = 1\n",
    "shape = ntk_exp.shape\n",
    "ind = 2\n",
    "invshape = shape[ind:] + shape[:ind]\n",
    "for k in shape[ind:]:\n",
    "  prod *= k\n",
    "ntk_exp = ntk_exp.reshape(prod, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H = np.tensordot(ntk_exp, Mr_exp, axes=[[2,3],[0,1]])\n",
    "H = Mr_collapsed @ ntk_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing $D$, defined as\n",
    "$$\n",
    "D = M_R^{-1} A^T C_Y^{-1} y - f_0 = K - f_0 \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing K\n",
    "Cinv_y = np.tensordot(Cinv_exp, y_exp, axes=[[1],[0]])\n",
    "A_Cinv_y = np.tensordot(fk_exp, Cinv_y, axes=[[0],[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_Cinv_y = A_Cinv_y.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = Mr_inv @ A_Cinv_y\n",
    "#K = np.tensordot(Mr_exp_inv, A_Cinv_y, axes=[[2,3], [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing f0\n",
    "f0 = nnpdf(xgrid_exp).numpy()\n",
    "f0 = f0.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the integrated solution\n",
    "$$\n",
    "f(t) = e^{-Ht} f_0 + \\left( I -  e^{-Ht} \\right) K \\,.\n",
    "$$\n",
    "Since $H$ is a rank-4 tensor, I'll write the expression above as follows\n",
    "$$\n",
    "f^{\\alpha}(t) = \\sum_{\\beta = 1}^{N_f} \\left[ \\left( e^{-Ht} \\right)^{\\alpha \\beta} f_0^{\\beta} + \\left( I - e^{-Ht} \\right)^{\\alpha \\beta} K^{\\beta} \\right]\\,.\n",
    "$$\n",
    "Note that, at LO in the theory of wide neural networks, the NTK is constant. Furthermore, the NTK is diagonal in the flavours\n",
    "$$\n",
    "\\Theta_{ij}^{\\alpha \\beta} = \\delta^{\\alpha \\beta} \\Theta_{ij} \\,.\n",
    "$$\n",
    "The integrated solution diagonalises in the flavour space\n",
    "$$\n",
    "f^{\\alpha}(t) = \\left[ \\left( e^{-Ht} \\right)^{\\alpha \\alpha} f_0^{\\alpha} + \\left( I - e^{-Ht} \\right)^{\\alpha \\alpha} K^{\\alpha} \\right]\\,.\n",
    "$$\n",
    "The expression above is fully diagonalized. Furthermore, we can introduce the eigenspace of the operator $H$. Recall that the operator $H$ is diagonal in the flavour space, and the diagonalization is performed with respect to the data space\n",
    "$$\n",
    "H^{\\alpha \\alpha} v^{\\alpha (k)} = \\lambda_k^{\\alpha} v^{\\alpha (k)} \\,.\n",
    "$$\n",
    "The vector $v^{\\alpha (k)} \\in \\mathbb{R}^{N_{dat}}$ is the eigenvector relative to the $k$-th eigenvalue $\\lambda_k^{\\alpha}$ for the matrix $H$ specified by flavour $\\alpha$. We can project the integrated equation in the eigenspace of $H$\n",
    "$$\n",
    "\\begin{split}\n",
    "f^{\\alpha}(t) & = \\sum_{k = 1}^{\\textrm{eig}(H)} \\left[  e^{- t \\lambda^{\\alpha}_k} \\; \\widetilde{f}_0^{\\alpha (k)} + \\left( 1 - e^{- t\\lambda^{\\alpha}_k} \\right) \\widetilde{K}^{\\alpha(k)} \\right] \\; v^{(k) \\alpha} \\\\\n",
    "& = \\sum_{k = 1}^{\\textrm{H}(\\Theta)} \\left[  C_1^{\\alpha(k)} + C_2^{\\alpha(k)} \\right] \\; v^{(k) \\alpha} \\,,\n",
    "\\end{split}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "C_1^{\\alpha(k)} = e^{- t \\lambda^{\\alpha}_k} \\; \\widetilde{f}_0^{\\alpha (k)} \\\\\n",
    "C_2^{\\alpha(k)} = \\left( 1 - e^{- t\\lambda^{\\alpha}_k} \\right) \\widetilde{K}^{\\alpha(k)}\n",
    "$$\n",
    "and $\\widetilde{f}_0$, $\\widetilde{K}$ are the component of the respective vectors in the eigenspace of $H$\n",
    "$$\n",
    "\\widetilde{f}_0^{\\alpha (k)} = \\left< f_0^{\\alpha}, v^{\\alpha(k)} \\right> \\,,\\\\\n",
    "\\widetilde{K}^{\\alpha (k)} = \\left< K^{\\alpha}, v^{\\alpha(k)} \\right> \\,.\n",
    "$$\n",
    "Note that in general $H$ is a 4-rank tensor although we only consider the diagonal elements in the flavour. Effectively, as it can be seen from the equations above, this means that we are neglecting all the mixing contributions for different flavours. This is consistent with the fact that, for large networks, the output should are independent of each other at initialization. Furthermore, the integrated equation above extends the statement also during training, as flavour $\\alpha$ does not receive any contribution from the other flavours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_flow_t_v2(t):\n",
    "  eigvals, eigvecs = np.linalg.eig(H)\n",
    "  eigvals = eigvals.real\n",
    "  eigvecs = eigvecs.real\n",
    "\n",
    "  f0_tilde = [np.dot(f0, eigvecs[:, k]) for k in range(eigvals.size)]\n",
    "  K_tilde = [np.dot(K, eigvecs[:, k]) for k in range(eigvals.size)]\n",
    "\n",
    "  output = np.zeros(shape=K.shape[0])\n",
    "  for k in range(eigvals.size):\n",
    "      C1_k = f0_tilde[k] * np.exp(- eigvals[k] * t)\n",
    "      C2_k = (1 - np.exp(- eigvals[k] * t) ) * K_tilde[k]\n",
    "      output = np.add(output, (C1_k + C2_k) * eigvecs[:,k])\n",
    "\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "integrated_data = namedtuple('integrated_data', ['Coefficients', 'Eigenvectors', 'Eigenvalues'])\n",
    "\n",
    "def integrate_flow_t(t):\n",
    "  output_by_flavour = defaultdict(list)\n",
    "  coeffs_eig_by_flavour = defaultdict(list)\n",
    "\n",
    "  for alpha in range(f0.shape[1]):\n",
    "    output = np.zeros(shape=ntk_exp.shape[1])\n",
    "    for beta in range(f0.shape[1]):\n",
    "\n",
    "      eigval, eigvec = np.linalg.eig(\n",
    "        0.5*(H[alpha, :, beta, :] + H[alpha, :, beta, :].T)\n",
    "             )\n",
    "      print(eigval)\n",
    "      eigval = abs(eigval.real)\n",
    "      eigvec = eigvec.real\n",
    "\n",
    "      \n",
    "      f0_tilde = [np.dot(f0[:,beta], eigvec[:, k]) for k in range(eigval.size)]\n",
    "      K_tilde = [np.dot(K[beta,:], eigvec[:, k]) for k in range(eigval.size)]\n",
    "      \n",
    "      #coefficients_collector = []\n",
    "      \n",
    "      for k in range(eigval.size):\n",
    "        C1_k = f0_tilde[k] * np.exp(- eigval[k] * t)\n",
    "        C2_k = (1 - np.exp(- eigval[k] * t) ) * K_tilde[k]\n",
    "        output = np.add(output, (C1_k + C2_k) * eigvec[:,k])\n",
    "        #coefficients_collector.append(C1_k + C1_k)\n",
    "      \n",
    "      #coefficients_eig = integrated_data(\n",
    "      #  Coefficients=coefficients_collector,\n",
    "      #  Eigenvectors=eigvec,\n",
    "      #  Eigenvalues=eigval\n",
    "      #)\n",
    "\n",
    "      #coeffs_eig_by_flavour[flavour_map[alpha]] = coefficients_eig\n",
    "\n",
    "    output_by_flavour[flavour_map[alpha]] = output\n",
    "\n",
    "  return  output_by_flavour#, coeffs_eig_by_flavour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 100000\n",
    "out = integrate_flow_t_v2(t)\n",
    "out = out.reshape(shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.interpolate as scint\n",
    "xnew = np.linspace(xgrid_exp.min(), xgrid_exp.max(), 300)  \n",
    "interp = scint.CubicSpline(xgrid_exp, out[4])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(xnew, interp(xnew), color='green', label='Integrated solution')\n",
    "#ax.set_title(f\"Integrated solution after t = {t}\")\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel(r'$T(t)$')\n",
    "#ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnpdf_doc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
