{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validphys.api import API\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from validphys.fkparser import load_fktable\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1341351341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequential_model(outputs=1, \n",
    "                   input_layer=None, \n",
    "                   nlayers=2, \n",
    "                   units=[100,100],\n",
    "                   seed=seed,\n",
    "                   **kwargs):\n",
    "  \"\"\"\n",
    "  Create a tensorflow sequential model where all intermediate layers have the same size\n",
    "  This function accepts an already constructed layer as the input.\n",
    "\n",
    "  All hidden layers will have the same number of nodes for simplicity\n",
    "\n",
    "  Arguments:\n",
    "      outputs: int (default=1)\n",
    "          number of output nodes (how many flavours are we training)\n",
    "      input_layer: KerasTensor (default=None)\n",
    "          if given, sets the input layer of the sequential model\n",
    "      nlayers: int\n",
    "          number of hidden layers of the network\n",
    "      units: int\n",
    "          number of nodes of every hidden layer in the network\n",
    "      activation: str\n",
    "          activation function to be used by the hidden layers (ex: 'tanh', 'sigmoid', 'linear')\n",
    "  \"\"\"\n",
    "  if len(units) != nlayers:\n",
    "      raise Exception(\"The length of units must match the number of layers.\")\n",
    "  \n",
    "  if kwargs.get('kernel_initializer'):\n",
    "      kernel_initializer = kwargs['kernel_initializer']\n",
    "  else:\n",
    "      kernel_initializer = tf.keras.initializers.HeNormal\n",
    "\n",
    "  if kwargs.get('activation_list'):\n",
    "      activation_list = kwargs['activation_list']\n",
    "      if len(units) != len(activation_list):\n",
    "          raise Exception(\"The length of the activation list must match the number of layers.\")\n",
    "  else:\n",
    "      activation_list = ['tanh', 'tanh']\n",
    "\n",
    "  if kwargs.get('output_func'):\n",
    "      output_func = kwargs['output_func']\n",
    "  else:\n",
    "      output_func = 'linear'\n",
    "  \n",
    "  if kwargs.get('name'):\n",
    "      name = kwargs['name']\n",
    "  else:\n",
    "      name = 'pdf'\n",
    "  \n",
    "  model = tf.keras.models.Sequential(name=name)\n",
    "  if input_layer is not None:\n",
    "      model.add(input_layer)\n",
    "  for layer in range(nlayers):\n",
    "      model.add(tf.keras.layers.Dense(units[layer], \n",
    "                                      activation=activation_list[layer],\n",
    "                                      kernel_initializer=kernel_initializer(seed=seed - layer),\n",
    "                                      ),\n",
    "      )\n",
    "  model.add(tf.keras.layers.Dense(outputs, \n",
    "                                  activation=output_func, \n",
    "                                  kernel_initializer=tf.keras.initializers.HeNormal(seed=seed - nlayers)\n",
    "                                  ))\n",
    "\n",
    "  return model\n",
    "\n",
    "def compute_ntk(model, input):\n",
    "  grad = []\n",
    "  for x in tf.convert_to_tensor(input):\n",
    "    with tf.GradientTape() as tape:\n",
    "      x = tf.reshape(x, shape=(-1,1))\n",
    "      #tape.watch(x)\n",
    "      pred = model(x)\n",
    "\n",
    "    # compute gradients df(x)/dtheta\n",
    "    g = tape.gradient(pred, model.trainable_variables)\n",
    "    # concatenate the gradients of all trainable variables,\n",
    "    # not discriminating between weights and biases\n",
    "    size_g = len(g)\n",
    "    g_minus_out = tf.concat([tf.reshape(g[i], shape=(-1,1)) for i in range(size_g - 2)], axis=0)\n",
    "    g = np.array([\n",
    "      np.concatenate([g_minus_out, \n",
    "                      tf.reshape(g[-2][:,i],  shape=(-1,1)), \n",
    "                      tf.reshape(g[-1][i],  shape=(-1,1))],\n",
    "                      axis=0\n",
    "                      )\n",
    "      for i in range(pred.shape[1])\n",
    "    ])\n",
    "    grad.append(g)\n",
    "\n",
    "  grad = np.array(grad)\n",
    "  ntk = tf.einsum('aikl,bjkl->ijab', grad, grad)\n",
    "  return ntk\n",
    "\n",
    "\n",
    "def produce_ntk_Y(ntk, start_grid_by_exp, grid_size_by_exp, fk_table_list, total_ndata, start_proc_by_exp, index):\n",
    "  # Constructing ntk_Y\n",
    "  sub_mats = defaultdict(list)\n",
    "\n",
    "  for exp_name_1, alpha in start_grid_by_exp.items():\n",
    "    for exp_name_2, beta in start_grid_by_exp.items():\n",
    "      # Take the submatrix of the NTK in data space\n",
    "      ntk_red = ntk[:, :, alpha : alpha + grid_size_by_exp[exp_name_1], beta : beta + grid_size_by_exp[exp_name_2]].numpy()\n",
    "      fk_I = fk_table_list[exp_name_1]\n",
    "      fk_J = fk_table_list[exp_name_2]\n",
    "      start_locs = (start_proc_by_exp[exp_name_1], start_proc_by_exp[exp_name_2]) # Wrong, this should have that index in the data space, not in the grid spaces\n",
    "      #print(\"-----------------------------\")\n",
    "      #print(f\"{fk_I.shape} - {ntk_red.shape} - {fk_J.shape}\")\n",
    "      fk_ntk = np.tensordot(fk_I, ntk_red, axes=[[1,2],[0,2]])\n",
    "      fk_ntk_fk = np.tensordot(fk_ntk, fk_J, axes=[[1,2],[1,2]])\n",
    "      sub_mats[start_locs] = fk_ntk_fk\n",
    "\n",
    "  result = np.zeros((total_ndata, total_ndata), dtype=np.float32)\n",
    "  for locs, mat in sub_mats.items():\n",
    "      xsize, ysize = mat.shape\n",
    "      print(mat.shape)\n",
    "      print(\"------------------------\")\n",
    "      print(f\"x| {locs[0]} : {locs[0] + xsize}\")\n",
    "      print(f\"y| {locs[1]} : {locs[1] + ysize}\")\n",
    "      print(locs)\n",
    "      print(mat)\n",
    "      result[locs[0] : locs[0] + xsize, locs[1] : locs[1] + ysize] = mat\n",
    "\n",
    "  return pd.DataFrame(result, index=index, columns=index), sub_mats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_inputs = [\n",
    "  #{'dataset': 'NMC_NC_NOTFIXED_DW_EM-F2', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'NMC_NC_NOTFIXED_P_EM-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'SLAC_NC_NOTFIXED_P_DW_EM-F2', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  #{'dataset': 'SLAC_NC_NOTFIXED_D_DW_EM-F2', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  #{'dataset': 'BCDMS_NC_NOTFIXED_P_DW_EM-F2', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  #{'dataset': 'BCDMS_NC_NOTFIXED_D_DW_EM-F2', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  #{'dataset': 'CHORUS_CC_NOTFIXED_PB_DW_NU-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  #{'dataset': 'CHORUS_CC_NOTFIXED_PB_DW_NB-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  #{'dataset': 'NUTEV_CC_NOTFIXED_FE_DW_NU-SIGMARED', 'cfac': ['MAS'], 'frac': 0.75, 'variant': 'legacy'},\n",
    "  #{'dataset': 'NUTEV_CC_NOTFIXED_FE_DW_NB-SIGMARED', 'cfac': ['MAS'], 'frac': 0.75, 'variant': 'legacy'},\n",
    "  #{'dataset': 'HERA_NC_318GEV_EM-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  #{'dataset': 'HERA_NC_225GEV_EP-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  #{'dataset': 'HERA_NC_251GEV_EP-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  #{'dataset': 'HERA_NC_300GEV_EP-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  #{'dataset': 'HERA_NC_318GEV_EP-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  #{'dataset': 'HERA_CC_318GEV_EM-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  #{'dataset': 'HERA_CC_318GEV_EP-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  #{'dataset': 'HERA_NC_318GEV_EAVG_CHARM-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  #{'dataset': 'HERA_NC_318GEV_EAVG_BOTTOM-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dict = dict(\n",
    "    dataset_inputs=dataset_inputs,\n",
    "    metadata_group=\"nnpdf31_process\",\n",
    "    use_cuts='internal',\n",
    "    datacuts={'t0pdfset': '240701-02-rs-nnpdf40-baseline', 'q2min': 3.49, 'w2min': 12.5},\n",
    "    theoryid=40000000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_data = API.procs_data(**common_dict)\n",
    "groups_index = API.groups_index(**common_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "fk_table_list = defaultdict(list)\n",
    "x_grid_list = defaultdict(list)\n",
    "Y = []\n",
    "total_ndata_wc = 0\n",
    "total_grid_size = 0\n",
    "start_grid_by_exp = defaultdict(list)\n",
    "grid_size_by_exp = defaultdict(list)\n",
    "start_proc_by_exp = defaultdict(list)\n",
    "\n",
    "for idx_proc, group_proc in enumerate(groups_data):\n",
    "  for idx_exp, exp_set in enumerate(group_proc.datasets):\n",
    "\n",
    "    fkspecs = exp_set.fkspecs\n",
    "    cuts = exp_set.cuts\n",
    "    ndata = exp_set.load_commondata().ndata\n",
    "    fk_table = load_fktable(fkspecs[0])\n",
    "    fk_table_wc = fk_table.with_cuts(cuts)\n",
    "    x_grid = fk_table_wc.xgrid\n",
    "\n",
    "    Y.append(exp_set.load_commondata().central_values.to_numpy())\n",
    "    fk_table_list[exp_set.name] = fk_table_wc.get_np_fktable()\n",
    "    x_grid_list[exp_set.name] = x_grid\n",
    "    start_proc_by_exp[exp_set.name] = total_ndata_wc\n",
    "    start_grid_by_exp[exp_set.name] = total_grid_size\n",
    "    grid_size_by_exp[exp_set.name] = x_grid.shape[0]\n",
    "    total_grid_size += x_grid.shape[0]\n",
    "    total_ndata_wc += ndata\n",
    "#print(f\"Total number of points after cuts: {total_ndata_wc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate NNPDF model\n",
    "nnpdf = generate_sequential_model(outputs=9, nlayers=2, units=[28, 20],seed=seed, name='NNPDF', kernel_initializer=tf.keras.initializers.GlorotNormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index for NTK\n",
    "grid_index_array = []\n",
    "grid_index_id = []\n",
    "counter = 0\n",
    "for set, x_grid in x_grid_list.items():\n",
    "  counter += x_grid.size\n",
    "  for id, x in enumerate(x_grid):\n",
    "    grid_index_array.append(set)\n",
    "    grid_index_id.append(id)\n",
    "\n",
    "grid_multi_index = pd.MultiIndex.from_arrays([grid_index_array, grid_index_id], names=('dataset','id'))\n",
    "\n",
    "x_grid_total = np.concatenate([grid for grid in x_grid_list.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntk = compute_ntk(nnpdf, x_grid_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204, 204)\n",
      "------------------------\n",
      "x| 204 : 408\n",
      "y| 204 : 408\n",
      "(204, 204)\n",
      "[[7.32335075 7.47860843 7.5413762  ... 6.48483865 6.4516079  6.3897897 ]\n",
      " [7.47860843 7.63940526 7.70472913 ... 6.67542857 6.64252079 6.58051269]\n",
      " [7.5413762  7.70472913 7.77125908 ... 6.75995643 6.72732461 6.66539844]\n",
      " ...\n",
      " [6.48483865 6.67542857 6.75995643 ... 6.996914   6.9917984  6.96357986]\n",
      " [6.4516079  6.64252079 6.72732461 ... 6.9917984  6.98730612 6.95988385]\n",
      " [6.3897897  6.58051269 6.66539844 ... 6.96357986 6.95988385 6.93354636]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (204,204) into shape (33,33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[299], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ntk_y, submat \u001b[38;5;241m=\u001b[39m \u001b[43mproduce_ntk_Y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mntk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_grid_by_exp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_size_by_exp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfk_table_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_ndata_wc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_proc_by_exp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[291], line 121\u001b[0m, in \u001b[0;36mproduce_ntk_Y\u001b[0;34m(ntk, start_grid_by_exp, grid_size_by_exp, fk_table_list, total_ndata, start_proc_by_exp, index)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28mprint\u001b[39m(locs)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mprint\u001b[39m(mat)\n\u001b[0;32m--> 121\u001b[0m     \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlocs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mxsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mysize\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m mat\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(result, index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mindex), sub_mats\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (204,204) into shape (33,33)"
     ]
    }
   ],
   "source": [
    "ntk_y, submat = produce_ntk_Y(ntk, start_grid_by_exp, grid_size_by_exp, fk_table_list, total_ndata_wc, start_proc_by_exp, groups_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 33)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntk_y.xs(level='dataset', key='SLAC_NC_NOTFIXED_P_DW_EM-F2').T.xs(level='dataset', key='SLAC_NC_NOTFIXED_P_DW_EM-F2').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.32335075, 7.47860843, 7.5413762 , ..., 6.48483865, 6.4516079 ,\n",
       "        6.3897897 ],\n",
       "       [7.47860843, 7.63940526, 7.70472913, ..., 6.67542857, 6.64252079,\n",
       "        6.58051269],\n",
       "       [7.5413762 , 7.70472913, 7.77125908, ..., 6.75995643, 6.72732461,\n",
       "        6.66539844],\n",
       "       ...,\n",
       "       [6.48483865, 6.67542857, 6.75995643, ..., 6.996914  , 6.9917984 ,\n",
       "        6.96357986],\n",
       "       [6.4516079 , 6.64252079, 6.72732461, ..., 6.9917984 , 6.98730612,\n",
       "        6.95988385],\n",
       "       [6.3897897 , 6.58051269, 6.66539844, ..., 6.96357986, 6.95988385,\n",
       "        6.93354636]])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submat[(0,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "x| 0 : 204\n",
      "y| 0 : 204\n",
      "(0, 0)\n",
      "[[7.32335075 7.47860843 7.5413762  ... 6.48483865 6.4516079  6.3897897 ]\n",
      " [7.47860843 7.63940526 7.70472913 ... 6.67542857 6.64252079 6.58051269]\n",
      " [7.5413762  7.70472913 7.77125908 ... 6.75995643 6.72732461 6.66539844]\n",
      " ...\n",
      " [6.48483865 6.67542857 6.75995643 ... 6.996914   6.9917984  6.96357986]\n",
      " [6.4516079  6.64252079 6.72732461 ... 6.9917984  6.98730612 6.95988385]\n",
      " [6.3897897  6.58051269 6.66539844 ... 6.96357986 6.95988385 6.93354636]]\n",
      "------------------------\n",
      "x| 0 : 204\n",
      "y| 34 : 67\n",
      "(0, 34)\n",
      "[[7.29964799 7.35032745 7.18358689 ... 6.13826102 6.1390513  6.13896548]\n",
      " [7.46805088 7.52045945 7.3539357  ... 6.32310934 6.32420883 6.32446495]\n",
      " [7.53805164 7.59125137 7.42534217 ... 6.40554322 6.40680911 6.40725213]\n",
      " ...\n",
      " [6.78608867 6.84643024 6.78736996 ... 6.72801395 6.73563438 6.74369334]\n",
      " [6.75920143 6.81961271 6.76302421 ... 6.72520995 6.73296133 6.74117884]\n",
      " [6.70438522 6.76469565 6.71138565 ... 6.70072358 6.70861513 6.717006  ]]\n",
      "------------------------\n",
      "x| 34 : 67\n",
      "y| 0 : 204\n",
      "(34, 0)\n",
      "[[7.29964799 7.46805088 7.53805164 ... 6.78608867 6.75920143 6.70438522]\n",
      " [7.35032745 7.52045945 7.59125137 ... 6.84643024 6.81961271 6.76469565]\n",
      " [7.18358689 7.3539357  7.42534217 ... 6.78736996 6.76302421 6.71138565]\n",
      " ...\n",
      " [6.13826102 6.32310934 6.40554322 ... 6.72801395 6.72520995 6.70072358]\n",
      " [6.1390513  6.32420883 6.40680911 ... 6.73563438 6.73296133 6.70861513]\n",
      " [6.13896548 6.32446495 6.40725213 ... 6.74369334 6.74117884 6.717006  ]]\n",
      "------------------------\n",
      "x| 34 : 67\n",
      "y| 34 : 67\n",
      "(34, 34)\n",
      "[[7.35883983 7.41332725 7.26988533 ... 6.45037807 6.452941   6.45494206]\n",
      " [7.41332725 7.46835602 7.32485254 ... 6.50878959 6.5114433  6.51354404]\n",
      " [7.26988533 7.32485254 7.19141338 ... 6.46029986 6.46342369 6.46610014]\n",
      " ...\n",
      " [6.45037807 6.50878959 6.46029986 ... 6.47667253 6.484466   6.49277642]\n",
      " [6.452941   6.5114433  6.46342369 ... 6.484466   6.49229786 6.50065329]\n",
      " [6.45494206 6.51354404 6.46610014 ... 6.49277642 6.50065329 6.50906165]]\n"
     ]
    }
   ],
   "source": [
    "result = np.zeros((total_ndata_wc, total_ndata_wc), dtype=np.float32)\n",
    "for locs, mat in submat.items():\n",
    "    xsize, ysize = mat.shape\n",
    "    print(\"------------------------\")\n",
    "    print(f\"x| {locs[0]} : {locs[0] + xsize}\")\n",
    "    print(f\"y| {locs[1]} : {locs[1] + ysize}\")\n",
    "    print(locs)\n",
    "    print(mat)\n",
    "    result[locs[0] : locs[0] + xsize, locs[1] : locs[1] + ysize] = mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.323351"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_ndata_wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnpdf_doc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
