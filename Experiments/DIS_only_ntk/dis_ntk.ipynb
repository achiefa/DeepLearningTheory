{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validphys.api import API\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from validphys.fkparser import load_fktable\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1341351341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequential_model(outputs=1, \n",
    "                   input_layer=None, \n",
    "                   nlayers=2, \n",
    "                   units=[100,100],\n",
    "                   seed=seed,\n",
    "                   **kwargs):\n",
    "  \"\"\"\n",
    "  Create a tensorflow sequential model where all intermediate layers have the same size\n",
    "  This function accepts an already constructed layer as the input.\n",
    "\n",
    "  All hidden layers will have the same number of nodes for simplicity\n",
    "\n",
    "  Arguments:\n",
    "      outputs: int (default=1)\n",
    "          number of output nodes (how many flavours are we training)\n",
    "      input_layer: KerasTensor (default=None)\n",
    "          if given, sets the input layer of the sequential model\n",
    "      nlayers: int\n",
    "          number of hidden layers of the network\n",
    "      units: int\n",
    "          number of nodes of every hidden layer in the network\n",
    "      activation: str\n",
    "          activation function to be used by the hidden layers (ex: 'tanh', 'sigmoid', 'linear')\n",
    "  \"\"\"\n",
    "  if len(units) != nlayers:\n",
    "      raise Exception(\"The length of units must match the number of layers.\")\n",
    "  \n",
    "  if kwargs.get('kernel_initializer'):\n",
    "      kernel_initializer = kwargs['kernel_initializer']\n",
    "  else:\n",
    "      kernel_initializer = tf.keras.initializers.HeNormal\n",
    "\n",
    "  if kwargs.get('activation_list'):\n",
    "      activation_list = kwargs['activation_list']\n",
    "      if len(units) != len(activation_list):\n",
    "          raise Exception(\"The length of the activation list must match the number of layers.\")\n",
    "  else:\n",
    "      activation_list = ['tanh', 'tanh']\n",
    "\n",
    "  if kwargs.get('output_func'):\n",
    "      output_func = kwargs['output_func']\n",
    "  else:\n",
    "      output_func = 'linear'\n",
    "  \n",
    "  if kwargs.get('name'):\n",
    "      name = kwargs['name']\n",
    "  else:\n",
    "      name = 'pdf'\n",
    "  \n",
    "  model = tf.keras.models.Sequential(name=name)\n",
    "  if input_layer is not None:\n",
    "      model.add(input_layer)\n",
    "  for layer in range(nlayers):\n",
    "      model.add(tf.keras.layers.Dense(units[layer], \n",
    "                                      activation=activation_list[layer],\n",
    "                                      kernel_initializer=kernel_initializer(seed=seed - layer),\n",
    "                                      ),\n",
    "      )\n",
    "  model.add(tf.keras.layers.Dense(outputs, \n",
    "                                  activation=output_func, \n",
    "                                  kernel_initializer=tf.keras.initializers.HeNormal(seed=seed - nlayers)\n",
    "                                  ))\n",
    "\n",
    "  return model\n",
    "\n",
    "def compute_ntk(model, input):\n",
    "  grad = []\n",
    "  for x in tf.convert_to_tensor(input):\n",
    "    with tf.GradientTape() as tape:\n",
    "      x = tf.reshape(x, shape=(-1,1))\n",
    "      #tape.watch(x)\n",
    "      pred = model(x)\n",
    "\n",
    "    # compute gradients df(x)/dtheta\n",
    "    g = tape.gradient(pred, model.trainable_variables)\n",
    "    # concatenate the gradients of all trainable variables,\n",
    "    # not discriminating between weights and biases\n",
    "    size_g = len(g)\n",
    "    g_minus_out = tf.concat([tf.reshape(g[i], shape=(-1,1)) for i in range(size_g - 2)], axis=0)\n",
    "    g = np.array([\n",
    "      np.concatenate([g_minus_out, \n",
    "                      tf.reshape(g[-2][:,i],  shape=(-1,1)), \n",
    "                      tf.reshape(g[-1][i],  shape=(-1,1))],\n",
    "                      axis=0\n",
    "                      )\n",
    "      for i in range(pred.shape[1])\n",
    "    ])\n",
    "    grad.append(g)\n",
    "\n",
    "  grad = np.array(grad)\n",
    "  ntk = tf.einsum('aikl,bjkl->ijab', grad, grad)\n",
    "  return ntk\n",
    "\n",
    "\n",
    "def produce_ntk_Y(ntk, start_grid_by_exp, grid_size_by_exp, fk_table_list, total_ndata, start_proc_by_exp, index):\n",
    "  # Constructing ntk_Y\n",
    "  sub_mats = defaultdict(list)\n",
    "\n",
    "  for exp_name_1, alpha in start_grid_by_exp.items():\n",
    "    for exp_name_2, beta in start_grid_by_exp.items():\n",
    "      # Take the submatrix of the NTK in data space\n",
    "      ntk_red = ntk[:, :, alpha : alpha + grid_size_by_exp[exp_name_1], beta : beta + grid_size_by_exp[exp_name_2]].numpy()\n",
    "      fk_I = fk_table_list[exp_name_1]\n",
    "      fk_J = fk_table_list[exp_name_2]\n",
    "      start_locs = (start_proc_by_exp[exp_name_1], start_proc_by_exp[exp_name_2]) # Wrong, this should have that index in the data space, not in the grid spaces\n",
    "      #print(\"-----------------------------\")\n",
    "      #print(f\"{fk_I.shape} - {ntk_red.shape} - {fk_J.shape}\")\n",
    "      fk_ntk = np.tensordot(fk_I, ntk_red, axes=[[1,2],[0,2]])\n",
    "      fk_ntk_fk = np.tensordot(fk_ntk, fk_J, axes=[[1,2],[1,2]])\n",
    "      sub_mats[start_locs] = fk_ntk_fk\n",
    "\n",
    "  result = np.zeros((total_ndata, total_ndata), dtype=np.float32)\n",
    "  for locs, mat in sub_mats.items():\n",
    "      xsize, ysize = mat.shape\n",
    "      result[locs[0] : locs[0] + xsize, locs[1] : locs[1] + ysize] = mat\n",
    "\n",
    "  return pd.DataFrame(result, index=index, columns=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_inputs = [\n",
    "  #{'dataset': 'NMC_NC_NOTFIXED_DW_EM-F2', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'NMC_NC_NOTFIXED_P_EM-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'SLAC_NC_NOTFIXED_P_DW_EM-F2', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'SLAC_NC_NOTFIXED_D_DW_EM-F2', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'BCDMS_NC_NOTFIXED_P_DW_EM-F2', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'BCDMS_NC_NOTFIXED_D_DW_EM-F2', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'CHORUS_CC_NOTFIXED_PB_DW_NU-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'CHORUS_CC_NOTFIXED_PB_DW_NB-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'NUTEV_CC_NOTFIXED_FE_DW_NU-SIGMARED', 'cfac': ['MAS'], 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'NUTEV_CC_NOTFIXED_FE_DW_NB-SIGMARED', 'cfac': ['MAS'], 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'HERA_NC_318GEV_EM-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'HERA_NC_225GEV_EP-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'HERA_NC_251GEV_EP-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'HERA_NC_300GEV_EP-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'HERA_NC_318GEV_EP-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'HERA_CC_318GEV_EM-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'HERA_CC_318GEV_EP-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'HERA_NC_318GEV_EAVG_CHARM-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "  {'dataset': 'HERA_NC_318GEV_EAVG_BOTTOM-SIGMARED', 'frac': 0.75, 'variant': 'legacy'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dict = dict(\n",
    "    dataset_inputs=dataset_inputs,\n",
    "    metadata_group=\"nnpdf31_process\",\n",
    "    use_cuts='internal',\n",
    "    datacuts={'t0pdfset': '240701-02-rs-nnpdf40-baseline', 'q2min': 3.49, 'w2min': 12.5},\n",
    "    theoryid=40000000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_data = API.procs_data(**common_dict)\n",
    "groups_index = API.groups_index(**common_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "fk_table_list = defaultdict(list)\n",
    "x_grid_list = defaultdict(list)\n",
    "Y = []\n",
    "total_ndata_wc = 0\n",
    "total_grid_size = 0\n",
    "start_grid_by_exp = defaultdict(list)\n",
    "grid_size_by_exp = defaultdict(list)\n",
    "start_proc_by_exp = defaultdict(list)\n",
    "\n",
    "for idx_proc, group_proc in enumerate(groups_data):\n",
    "  for idx_exp, exp_set in enumerate(group_proc.datasets):\n",
    "\n",
    "    fkspecs = exp_set.fkspecs\n",
    "    cuts = exp_set.cuts\n",
    "    ndata = exp_set.load_commondata().ndata\n",
    "    fk_table = load_fktable(fkspecs[0])\n",
    "    fk_table_wc = fk_table.with_cuts(cuts)\n",
    "    x_grid = fk_table_wc.xgrid\n",
    "\n",
    "    Y.append(exp_set.load_commondata().central_values.to_numpy())\n",
    "    fk_table_list[exp_set.name] = fk_table_wc.get_np_fktable()\n",
    "    x_grid_list[exp_set.name] = x_grid\n",
    "    start_proc_by_exp[exp_set.name] = total_ndata_wc\n",
    "    start_grid_by_exp[exp_set.name] = total_grid_size\n",
    "    grid_size_by_exp[exp_set.name] = x_grid.shape[0]\n",
    "    total_grid_size += x_grid.shape[0]\n",
    "    total_ndata_wc += ndata\n",
    "#print(f\"Total number of points after cuts: {total_ndata_wc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate NNPDF model\n",
    "nnpdf = generate_sequential_model(outputs=9, nlayers=2, units=[28, 20],seed=seed, name='NNPDF', kernel_initializer=tf.keras.initializers.GlorotNormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index for NTK\n",
    "grid_index_array = []\n",
    "grid_index_id = []\n",
    "counter = 0\n",
    "for set, x_grid in x_grid_list.items():\n",
    "  counter += x_grid.size\n",
    "  for id, x in enumerate(x_grid):\n",
    "    grid_index_array.append(set)\n",
    "    grid_index_id.append(id)\n",
    "\n",
    "grid_multi_index = pd.MultiIndex.from_arrays([grid_index_array, grid_index_id], names=('dataset','id'))\n",
    "\n",
    "x_grid_total = np.concatenate([grid for grid in x_grid_list.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntk = compute_ntk(nnpdf, x_grid_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntk_y = produce_ntk_Y(ntk, start_grid_by_exp, grid_size_by_exp, fk_table_list, total_ndata_wc, start_proc_by_exp, groups_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental covariance matrix\n",
    "C = API.groups_covmat_no_table(**common_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  pd.testing.assert_index_equal(C.index, ntk_y.index)\n",
    "except AssertionError as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnpdf_doc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
