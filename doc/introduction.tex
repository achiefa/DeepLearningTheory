\section{Introduction}
\label{sec:intro}

Parton Distribution Functions (PDFs) play a crucial role in describing
experimental data at hadron colliders and in gaining insights into the internal
structure of the proton. As we have now entered the high-precision era of
particle physics, the need for robust PDF determinations with reliable
uncertainty quantification has become increasingly important for both Standard
Model measurements and searches for new physics.

As non-perturbative objects, PDFs cannot be computed from first principles
but must be extracted from global analyses to experimental data. However, PDF
determination is a classic example of an \textit{inverse problem}, as it
involves inferring a continuous function from a finite set of data points. This
process is inherently ill-defined, and the limited amount of experimental
information prevents us from obtaining a unique solution to the problem. The
solution will inevitably depend on the assumptions made and on the prior
knowledge introduced to regularise the problem, either explicitly stated or
implicitly embedded in the fitting framework.

The complex nature of inverse problems has prompted the development of
sophisticated statistical methods, cutting-edge methodologies, and advanced
tools to tackle them. In general, PDF determinations can be broadly classified
into two main categories, depending on whether a specific functional form is
assumed for the PDFs or whether a non-parametric approach is adopted. Although
the former approach has been widely used in the literature, non-parametric
approaches based on Bayesian inference have been successfully applied to the
problem of PDF determination, even though in a limited scenario. For instance,
in Refs.~\cite{DelDebbio:2021whr,Candido:2024hjt} Gaussian Processes (GP) were
used to extract the non-singlet quark distribution $xT_3$ from deep-inelastic
scattering (DIS) data. The advantage of this Bayesian-based approach is that it
provides a rigorous framework where prior information and assumptions are spelled
out explicitly.

On the other hand, state-of-the-art PDF determinations rely on parametric approaches,
where a specific functional form is assumed for the PDFs at a given initial
scale $Q_0$. These functions forms are typically chosen to be flexible enough to
capture the main features of the PDFs, while their internal parameters are
optimised to reproduce the experimental data. Several
groups~\cite{NNPDF:2021njg,Ablat:2024hbm,Bailey:2020ooq,Alekhin:2017kpj} have
set the standard for PDF determinations through continuous refinement of their
global fits as new data and theoretical advances become available, with an
increasing emphasis on uncertainty quantification. Although these determinations
have been shown to perform incredibly well on a wide range of new experimental
data~\cite{Chiefa:2025loi}, the different methodological frameworks adopted by
the various groups lead to PDF sets whose differences are yet to be fully
understood~\cite{Harland-Lang:2024kvt}. These differences become significantly
visible when considering parameter determinations that are particularly
sensitive to the choice of the PDF set, thus on the central value and the
associated uncertainty. Examples of such parameters include the strong coupling
constant $\alpha_s$ and other Standard Model parameters (\ac{to be continued})

In this paper, we build upon the intents of
Refs.~\cite{DelDebbio:2021whr,Candido:2024hjt}, which aim at providing a
transparent statistical and sound framework for PDF determination, with all
underlying assumptions clearly stated. Here we focus on the NNPDF approach,
which pioneered the use of machine learning tools in the context of PDF
determination and has been continuously developed over the years~[\red{ref}]. The
NNPDF framework, which combines a Monte Carlo sampling of the experimental data
and a feed-forward neural network parameterisation of the PDFs, has been
validated through extensive studies, including closure tests~[\red{ref}], future
tests~[\red{ref}], and various benchmark studies~\cite{Harland-Lang:2024kvt}. Here,
we provide a complementary effort to these studies. A simplified but controlled
framework is adopted to revisit the methodology from first principles, aiming at
providing an explainable reformulation of its key aspects and making transparent
the assumptions that are often implicitly embedded in the fitting procedure. In
particular, in this work we focus on the training process. We present this study
as an exploration of foundational aspects, with the understanding that further
investigations will be needed to extend these ideas to the full complexity of
modern global PDF fits.

The remainder of this paper is organized as follows. In ...
