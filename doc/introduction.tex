\section{Introduction}
\label{sec:intro}

Parton Distribution Functions (PDFs) are a central ingredient in describing
experimental data at hadron colliders and in gaining insights into the internal
structure of the proton. The high-precision era of particle physics that we are
now witnessing calls for equally precise theoretical predictions. Since PDFs are
a key ingredient in these predictions, the need for robust PDF determinations
with reliable uncertainty quantification has become increasingly important for
both Standard Model measurements and searches for new physics.

PDFs are typically extracted from global analyses of experimental and lattice data. 
Their determination is a classic example of an \textit{inverse problem}, as it
involves inferring a continuous function from a finite set of data points. This
process is inherently ill-defined, and the limited amount of experimental
information prevents us from obtaining a unique solution to the problem. The
solution will inevitably depend on the assumptions made and on the prior
knowledge introduced to regularise the problem, either explicitly stated or
implicitly embedded in the fitting framework.

The complex nature of inverse problems has prompted the development of
sophisticated statistical methods and
tools to tackle them. In general, PDF determinations can be broadly classified
into two main categories, depending on whether a specific functional form is
assumed for the PDFs or whether a non-parametric approach is adopted. Although
the former approach has been widely used in the literature, non-parametric
approaches based on Bayesian inference have been successfully applied to the
problem of PDF determination, albeit in a limited
scenario~\cite{DelDebbio:2021whr,Candido:2024hjt,Medrano:2025cmg}.
Bayesian-based approaches are promising tools that ensure a rigorous framework
where prior information and assumptions are spelled out explicitly. Yet, a
global PDF determination based on these methods has not yet been attempted, and the 
impact of the prior needs to be carefully studied in these frameworks. 

On the other hand, state-of-the-art PDF determinations rely on parametric
approaches, where a specific functional form is assumed for the PDFs at a given
initial scale $Q_0$. These functions are typically chosen to be flexible enough
to capture the main features of the PDFs, while their internal parameters are
optimised to reproduce the experimental data. Several
groups~\cite{NNPDF:2021njg,Ablat:2024hbm,Bailey:2020ooq,Alekhin:2017kpj} have
set the standard for PDF determinations through continuous refinement of their
global fits as new data and theoretical advances become available, with an
increasing emphasis on uncertainty quantification. Although these determinations
have been shown to perform well on a wide range of new experimental
data~\cite{Chiefa:2025loi}, the different methodological frameworks adopted by
the various groups lead to PDF sets whose differences are yet to be fully
understood~\cite{Harland-Lang:2024kvt,PDF4LHCWorkingGroup:2022cjn}. These
differences become significantly visible when considering parameter
determinations that are particularly sensitive to the choice of the PDF set,
both on the central values and, most importantly, on the associated uncertainties
(see Refs.~\cite{ATLAS:2023lhg,CMS:2024ony,ATLAS:2023fsi} for some recent
examples).

In this work, we build upon the work of
Refs.~\cite{DelDebbio:2021whr,Candido:2024hjt}, which aims at providing a
sound statistical framework for PDF determination, with all
underlying assumptions clearly stated. We focus on the NNPDF
methodology~\cite{NNPDF:2021njg}, which pioneered the use of ML
tools in the context of PDF determinations and has been validated through
extensive studies over the
years~\cite{DelDebbio:2021whr,Barontini:2025lnl,Cruz-Martinez:2021rgy}. It
combines a Monte Carlo sampling of the experimental data and a feed-forward
neural network parameterisation of the PDFs. We adopt a simplified 
framework to analyse the training process,
aiming at providing a quantitative description of its key aspects, and making
transparent the assumptions that are often implicitly embedded in the fitting
procedure.

We demonstrate that the training dynamics of a neural network can be fully
reformulated in functional space, leading to an interpretable
description of the learning process. We show that the training dynamics is
dictated by the Neural Tangent Kernel
(NTK)~\cite{DBLP:journals/corr/abs-1806-07572}, which encodes and factorises the
dependence on the architecture and the parameters of the neural network. In
fact, the spectral properties of the NTK provide a powerful lens through which
we can understand the learning process: only the directions that are orthogonal to 
the kernel of the NTK are actually learned in the training process. 
At initialisation, the NTK is characterised
by a wide spectrum of eigenvalues, with only a few large eigenvalues being
significantly different from zero. During the training process, the hierarchy 
in the NTK spectrum is
preserved, but eigenvalues that were initially subleading, or zero, grow in magnitude.
Since the only directions that contribute to the learning process are those
associated to the non-zero eigenvalues, with the actual value of the eigenvalue 
setting the learning speed along the corresponding eigenvector direction, 
the growth of some eigenvalues indicates that new features in the
functional space emerge during training and that the network thus becomes capable of
representing more complex functions.

Another key result of this work is that, after an initial transient phase where
the NTK evolves significantly, the training process enters a second regime where
the NTK becomes approximately constant. This regime is often referred to as
\textit{lazy training} in the Machine Learning
literature~\cite{DBLP:journals/corr/abs-1806-07572}, and it has important
implications for the training dynamics. In this regime, we show that the
training process can be described analytically, allowing us to obtain a single
and clean closed-form expression for the output of the network at any point
during training. The main result of this analysis is summarised in Eq.~\eqref{eq:AnalyticSol}
which we report here:
\[
    f_{t}
        = U(t) f_{0} + V(t) Y\, , 
\]
where $f_{t}$ is the network output at training time $t$, $f_{0}$
is the initial output at $t=0$, $Y$ are the training data,
and $U(t)$ and $V(t)$ are time-dependent matrices that depend
on the NTK and are computed explicitly in Sect.~\ref{sec:LazyTraining}.
It is interesting to remark that this expression decomposes into two
contributions: one that depends on the initial condition and another that
depends on the data, thus making explicit the role of prior information and of
the experimental measurements in shaping the final result. This analytical expression
also allows us to compute the evolution of the covariance of the network output
during training, providing a quantitative description of how uncertainties
are propagated from the data to the fitted function.
Although applicable
only when the NTK reaches stability, this analytical description serves is a
powerful tool to bridge the gap between the parametric regression approach
adopted in NNPDF and other methods for solving inverse problems that are receiving growing
attention in the community.

While derived in a simplified setting -- considering a single PDF flavor
combination with DIS data and vanilla gradient descent optimization -- we
present this study as an exploration of foundational aspects, with the
understanding that further investigations will be needed to extend these ideas
to the full complexity of modern global PDF fits. We particularly emphasise that
the present analysis is not limited to neural networks, but can be extended to
any functional parameterisation that undergoes a gradient-based training
process. It will be interesting to explore the properties of the NTK together
with its spectral structure in more realistic PDF fits, translating the differences
between various fitting methodologies in terms of the NTK. We leave these
studies to future work.

The remainder of this paper is organized as follows. In Section~\ref{sec:Init}
the inverse problem of PDF determination is briefly reviewed in the simplified
case of theoretical predictions that depend linearly on the PDFs. We then review
some fundamental statistical aspects of the Neural Networks at initialisation,
which will be relevant in the rest of the paper. The training dynamics is then
discussed in Section~\ref{sec:Training}, where the learning process of the
neural network is reformulated in functional space by means of the NTK. The
implications of the \textit{lazy training} regime are used in
Section~\ref{sec:LazyTraining} to derive an analytical description of the
training process. Finally, we present our conclusions and outlook in
Section~\ref{sec:conclusions}.
