\section{Neural Networks and PDFs}
\label{sec:Init}


In the following, we prepare the ground for the study of the training dynamics
of neural networks used in the NNPDF framework. We start by briefly presenting
the inverse problem of PDF determination using data depending linearly on the PDFs,
setting the notation and introducing the statistical
vocabulary used in the rest of this study. We then discuss some
statistical aspects of the neural networks at initialisation, which will help us
understand the implications in the training process. These properties, derived
in the large-width limit~\cite{lee2019wide,jacot2018neural}, are analysed for the
specific architecture used in the NNPDF methodology. An exhaustive and detailed
review of wide-network properties is beyond the scope of this work, and the
reader is encouraged to refer to Ref.~\cite{Roberts:2021fes} for a comprehensive
review.

\subsection{The 1-dimensional regression problem of PDFs}
\label{subsec:inverse_problem}

The extraction of PDFs from experimental data is a classic example of an inverse
problem, namely the reconstruction of a function $f(x)$ from a finite set of
data points $Y_I$, where the index $I=1, \ldots, \ndat$\footnote{When omitting
the data index $I$, we will always assume $Y \in \mathbb{R}^{\ndat}$.}. In
particular, for this study, we will focus on DIS data, which depend linearly on
the function $f(x)$. The theoretical prediction for the data point $Y_I$ is
given by
\begin{equation}
    \label{eq:TheoryPred}
    T_I[f] = \sum_{i=1}^{\nflav} \int dx\, C_{Ii}(x) f_{i}(x)\, ,
\end{equation}
where $C_{Ii}(x)$ is a coefficient function, known to some given order in
perturbation theory, $i = 1, \ldots, \nflav$, labels the parton flavor, 
and $f_i(x)$ is the PDF (or set of PDFs) that we want to determine.

Attempting to determine a function $f$ in an infinite dimensional space of
solutions using a finite set of data is inherently ill-posed. The solution
inevitably depends on assumptions and prior knowledge -- conscious or not --
introduced to regularise the problem. Different methodologies, based either on
non-parametric methods or parametric regression, have been proposed to address
these challenges, yielding increasingly precise PDFs. Yet, despite
the longstanding effort to provide robust uncertainty quantification and
establish the relationships between different methodologies and their solutions,
some discrepancies remain unresolved, see, \eg, \cite{PDF4LHCWorkingGroup:2022cjn}. 
Understanding such differences between the
various approaches is thus crucial for precision physics.

Following the ideas highlighted in
Refs.~\cite{DelDebbio:2021whr,Candido:2024hjt}, the solution of the inverse
problem is conveniently phrased in a Bayesian framework. The functions $f_i$ are
promoted to stochastic processes; for any grid of points $x_{\alpha}$,
$\alpha=1, \ldots, \ngrid$, the vector $f_{i\alpha}=f_{i}(x_{\alpha})$ is a
vector of $\nflav\times\ngrid$ stochastic variables, for which we introduce a
{\em prior}\ distribution $p(f)$~\footnote{Following the same convention used for the
data, when omitting the grid index $\alpha$, and/or the flavor index $i$, we
will always refer to a vector $f \in \mathbb{R}^{\nflav\times\ngrid}$.}. In this
perspective, any fitting procedure is interpreted as a recipe that yields the
{\em posterior}\ distribution $\tilde{p}(f) = p(f | Y)$.
In this study, following the NNPDF methodology, probability distributions are represented by
ensembles of i.i.d. neural network replicas. So, for instance, the prior
distribution $p(f)$ is described by an ensemble
\begin{equation}
    \label{eq:RepDef}
    \left\{f^{(k)} \in \mathbb{R}^{\nflav\times\ngrid}; k=1, \ldots, \nreps\right\}\, ,
\end{equation}
drawn from the distribution $p$, so that
\begin{equation}
    \label{eq:ReplicaEnsemble}
    \mathbb{E}_{p}[O(f)] = \frac{1}{\nreps} \sum_{k=1}^{\nreps} O(f^{(k)})\, ,
\end{equation}
for any observable $O$ that is built from the PDFs.

The prior distribution $p(f)$ is defined by initializing a set of neural networks (NNs) 
replicas using a Glorot normal initializer~\cite{glorot2010understanding}. The result of this
initialisation is discussed below in Sec.~\ref{sec:NNinit}.

In order to account for the experimental uncertainties and propagate them to the
fitted PDFs, the NNPDF collaboration uses Monte Carlo replicas. For each
replica, labelled by the index $k$, a new set of data $Y^{(k)}$ is generated from an $\ndat$ dimensional
Gaussian distribution centred at the experimental central value $Y$, with the
covariance given by the experimental covariance matrix $C_Y$,
\begin{equation}
    \label{eq:ExpReplicaDistr}
    Y^{(k)} \sim \mathcal{N}\left(Y, C_Y\right)\, .
\end{equation}
Each replica $f^{(k)}$ is trained on its corresponding data set $Y^{(k)}$. We
denote the replicas at training time $t$ as $f^{(k)}_{t} \in
\mathbb{R}^{\nflav\times\ngrid}$. Stopping the training at time $T$, the
posterior probability distribution is represented by the set of 
{\em trained}\ replicas
$\left\{f^{(k)}_{T}\in \mathbb{R}^{\nflav\times\ngrid}; k=1, \ldots,
\nreps\right\}$, so that averages over the posterior distribution are computed
as
\begin{equation}
    \label{eq:PostEnsemble}
    \mathbb{E}_{\tilde{p}}[O(f)] = \frac{1}{\nreps} \sum_{k=1}^{\nreps}
        O\left(f^{(k)}_{T}\right)\, .
\end{equation}
All knowledge about the solution of the inverse problem, $f$, is encoded in the
posterior $\tilde{p}$ and is expressed as expectation values of observables $O$
using Eq.~\eqref{eq:PostEnsemble}. Let us stress once again that the expectation values
with respect to the prior and posterior distributions are both obtained by taking 
averages over replicas. The expectation value with respect to the prior is the average over
replicas at initialization. The expectation value with respect to the posterior is the average
over the replicas at training time $T$. 

Training may yield different posteriors depending on the initial network
configuration. To understand this dependence, we pause to examine the
statistical properties of network ensembles at initialization. This analysis
provides a quantitative insight into how prior knowledge embedded in the initialization
interacts with, and evolves throughout, the training process, as we show in
Sec.~\ref{sec:LazyTraining}.

\subsection{Neural Networks at Initialisation}
\label{sec:NNinit}

When initializing a neural network, the weights and biases -- which we denote
collectively as the {\em parameters}\ of the network -- are drawn from some
probability distribution. In the NNPDF formalism, the set of network parameters
at initialisation for each replica is an instance of i.i.d. stochastic
variables. More importantly, the probability distribution of the network
parameters induces a probability distribution for the output of the neural
networks at initialisation. It is well known that the probability distribution
of these outputs becomes approximately Gaussian when the size of the hidden
layers is increased~~\cite{Roberts:2021fes}. We call this limit the {\em large-network} limit.

As detailed in Ref.~\cite{NNPDF:2021njg}, the NNs used for the NNPDF fit have a
2-25-20-8 architecture, a $\tanh$ activation function, and are initialized using
a Glorot normal distribution~\cite{glorot2010understanding}. The preactivation
function of a neuron is denoted as $\phi^{(\ell)}_{i,\alpha} =
\phi^{(\ell)}_i(x_\alpha)$, where $\ell = 0, \ldots, L$, denotes the layer of the neuron, 
and, for each $\ell$, $i=1, \ldots, n_{\ell}$
identifies the neuron within the layer\footnote{We refer to $i$ as the {\em
neuron}\ index.}. Furthermore, $x_{\alpha}$ is the input to the NN, \ie, a point in the 
interval $[0,1]$. A grid of
$\ngrid=50$ points in $x$ is used to compute observables in the NNPDF formalism and in
this work we focus on the values of $f$ at those values of $x_\alpha$, where 
the index $\alpha = 1, \ldots, \ngrid$ labels the points on the grid. For
completeness, we list the values of $x_\alpha$ in Tab.~\ref{tab:Xvals}.

\begin{table}[ht]
    \centering
    \input{tabs/grid.tex}
    \caption{Values of $x_\alpha$ used in the NNPDF grids for the computation of
    observables. The points are equally spaced on a logarithmic scale
    for $\alpha = 1, \ldots, 27$, and linearly spaced for $\alpha > 27$.
    \label{tab:Xvals}}
\end{table}

The output of the neuron is identified by the pair $(\ell,i)$ is
$\rho^{(\ell)}_{i\alpha} = \tanh\left(\phi^{(\ell)}_{i\alpha}\right)$.
The parameters of the NN are the weights $w^{(\ell)}_{ij}$ and the biases $b^{(\ell)}_i$, which are
collectively denoted as $\theta_\mu$, where $\mu = 1, \ldots, P$, and the total number of parameters
is
\begin{equation}
    \label{eq:TotPar}
    P = \sum_{\ell=1}^{L} \left(n_{\ell} n_{\ell-1} + n_\ell\right)\, .
\end{equation}
The preactivation function in layer $(\ell+1)$ is a weighted average of the outputs of the neurons on 
the previous layer, namely
\begin{align}
    \label{eq:RecursionNN}
    \phi^{(\ell+1)}_{i\alpha} = \sum_{j=1}^{n_\ell} w^{(\ell+1)}_{ij} \rho^{(\ell)}_{j\alpha} + b^{(\ell+1)}_{i}\, .
\end{align}
The PDFs in the
so-called evolution basis are parametrized by the preactivation functions of the output layer $L$,
$x_\alpha f_i(x_\alpha)=A_i \phi^{(L)}_{i,\alpha}$, where the neuron index on the last layer,
$i=1, \ldots, 8$, labels the 
flavors.\footnote{For simplicity, we ignore the preprocessing function $x^{-\alpha_i} (1-x)^{\beta_i}$ that
is currently used in the NNPDF fits. While the preprocessing may be useful in speeding the training
it does not affect the current discussion.}
The input layer is identified by $\ell=0$ and the activation
function for that specific layer is the identity, so that
\begin{equation}
    \label{eq:InitLayerPhi}
    \rho^{(0)}_{i,\alpha} = \phi^{(0)}_{i,\alpha} = x_{i,\alpha} =
    \begin{cases}
        x_\alpha\, , \quad &\text{for}\ i=1\, ;\\
        \log\left(x_\alpha\right)\, , \quad &\text{for}\ i=2\, .
    \end{cases}
\end{equation}
In the following we refer to the preactivation functions as {\em fields}.

% ===================================
\begin{figure}[t!]
  \centering
  \includegraphics[width=0.95\textwidth]{section_2/weight_distribution.pdf}
  \caption{Sampled distribution of a selected weight as a function of the
  number of replicas. The red line represents the underlying Gaussian distribution
  from which the weights are drawn. As the number of replicas is increased the 
  distribution of the weight converges to the expected Gaussian.}
  \label{fig:weight_distribution}
\end{figure}
% ===================================
The Glorot normal initialiser draws each weight and bias of the NN from independent Gaussian
distributions, denoted $p_w$ and $p_b$ respectively, centred at zero and with variances
rescaled by the number of nodes in adjacent layers,
\begin{equation}
    \label{eq:RescaledGlorotVariances}
    \frac{C^{(\ell)}_{w}}{n_{\ell-1} + n_{\ell}}\, ,
    \quad \frac{C^{(\ell)}_{b}}{n_{\ell-1} + n_{\ell}}\, .
\end{equation}
Following the NNPDF prescription, we have $C^{(\ell)}_{w}=C^{(\ell)}_{b}=1$.
Figure~\ref{fig:weight_distribution} shows the binned distribution of one of the
weights in the network as a function of the number of replicas. Together with the
histogram, the underlying Gaussian, as dictated by the Glorot normal
initialisation, is also shown. The figure illustrates how the distribution of
the weights converges to the expected Gaussian as the number of replicas increases.

The probability distribution of the NN parameters induces a probability distribution for the
preactivations; the probability distribution of the fields in layer $\ell$, for given values of 
the field in the layer $\ell-1$ is
\begin{align}
    \label{eq:PreactAtInit}
    p\left(\phi^{(\ell)} | \phi^{(\ell-1)}\right)
      &= \int \mathcal{D}w\, p_w(w)\,
        \mathcal{D}b\, p_b(b)\, \prod_{i,\alpha}
        \delta\left(
          \phi^{(\ell)}_{i\alpha} - \sum_{j} w^{(\ell)}_{ij}
          \rho\left(\phi^{(\ell-1)}_{j\alpha}\right)
          - b^{(\ell)}_i
          \right)\, .
\end{align}
For clarity of writing, we will omit the condition in the probability distribution, and write 
simply $p(\phi^{(\ell)})$. 
Note that, here and in what follows, $p(\phi^{(\ell)})$ denotes the joint probability for all the
$n_{\ell}\times\ngrid$ components of $\phi^{(\ell)}$,
\begin{align}
    \label{eq:ExplIndices}
    p\left(\phi^{(\ell)}\right) = p\left(\phi^{(\ell)}_{1,\alpha_1}, \phi^{(\ell)}_{2,\alpha_1}, \ldots,
        \phi^{(\ell)}_{n_\ell,\alpha_1}, \phi^{(\ell)}_{1,\alpha_2}, \ldots, \phi^{(\ell)}_{n_\ell,\alpha_2},
        \ldots,
        \phi^{(\ell)}_{n_\ell,\ngrid}\right)\, .
\end{align}
This duality between parameter-space and function-space provides a powerful framework to study
the behaviour of an ensemble of NNs, and in particular the symmetry properties of the distribution
$p(\phi^{(\ell)})$ (see, \eg, Ref.~\cite{Maiti:2021fpy}). Working in parameter space, \ie, computing the
expectation values of correlators of fields as integrals over the NN parameters, one can readily
show that
\begin{align}
    \label{eq:NeurRotInv}
    \mathbb{E}\left[
        R_{i_1j_1} \phi^{(\ell)}_{j_1 \alpha_1} \ldots
        R_{i_nj_n} \phi^{(\ell)}_{j_n \alpha_n}
    \right] =
    \mathbb{E}\left[
        \phi^{(\ell)}_{i_1 \alpha_1} \ldots
        \phi^{(\ell)}_{i_n \alpha_n}
    \right]\, ,
\end{align}
where $R$ is an orthogonal matrix in $\text{SO}(n_{\ell})$. Eq.\eqref{eq:NeurRotInv} implies
that the probability distribution in Eq.~\eqref{eq:PreactAtInit} is also invariant under rotations,
and therefore it can only be a function of $\text{SO}(n_{\ell})$ invariants. Therefore
\begin{align}
    \label{eq:PriorAction}
    p\left(\phi^{(n_\ell)}\right) =
        \frac{1}{Z^{(\ell)}} \exp\left(-S\left[\phi^{(\ell)}_{\alpha_1}
            \cdot \phi^{(\ell)}_{\alpha_2}\right]\right)\, ,
\end{align}
where
\begin{align}
    \label{eq:PhiInvariant}
    \phi^{(\ell)}_{\alpha_1}
            \cdot \phi^{(\ell)}_{\alpha_2} =
    \sum_{i=1}^{n_\ell} \phi^{(\ell)}_{i \alpha_1} \phi^{(\ell)}_{i \alpha_2}\, .
\end{align}
The action can be expanded in powers of the invariant bilinear,
\begin{align}
    \label{eq:ExpandAction}
    S\left[\phi^{(\ell)}_{\alpha_1}
            \cdot \phi^{(\ell)}_{\alpha_2}\right] =
        \frac12 \gamma^{(\ell)}_{\alpha_1\alpha_2}
            \phi^{(\ell)}_{\alpha_1} \cdot \phi^{(\ell)}_{\alpha_2} +
            \frac{1}{8 n_{\ell-1}} \gamma^{(\ell)}_{\alpha_1\alpha_2,\alpha_3\alpha_4}
            \phi^{(\ell)}_{\alpha_1} \cdot \phi^{(\ell)}_{\alpha_2} \,
            \phi^{(\ell)}_{\alpha_3} \cdot \phi^{(\ell)}_{\alpha_4} + O(1/n_{\ell-1}^2)\, ,
\end{align}
so that the probability distribution is fully determined by the couplings 
$\gamma^{(\ell)}$.\footnote{ We have denoted {\em all}\ couplings by
  $\gamma^{{(\ell)}}$. Different couplings are identified by the number of
  indices, so that $\gamma^{(\ell)}_{\alpha_1\alpha_2}$ is a two-point coupling,
  $\gamma^{(\ell)}_{\alpha_1\alpha_2,\alpha_3\alpha_4}$ is a four-point coupling,
  etc. } 
In Eq.~\eqref{eq:ExpandAction}, we have factored out inverse powers of
$n_\ell$ for each coupling. With this convention, and with the scaling of the
parameters variances in Eq.~\eqref{eq:RescaledGlorotVariances}, the couplings in
the action are all $O(1)$ in the limit where $n_\ell\to\infty$. As a
consequence, the probability distribution at initialisation is a
multidimensional Gaussian at leading order -- \ie, $\mathcal{O}(1)$ -- in
$1/n_\ell$, with quartic corrections that are $O(1/n_\ell)$, while higher powers
of the invariant bilinear are suppressed by higher powers of the width of the
layer. This power counting defines an effective field theory, where deviations
from Gaussianity can be computed in perturbation theory to any given order in
$1/n_\ell$, see, \eg\, Ref.~\cite{Roberts:2021fes,Chiefa:2026TBA} for a detailed presentation
of these ideas. While the actual calculations become rapidly cumbersome, the
conceptual framework is straightforward.

At leading order, the second and fourth cumulant are respectively
\begin{align}
    &\langle \phi^{(\ell)}_{i_1,\alpha_1} \phi^{(\ell)}_{i_2,\alpha_2}\rangle
      = \delta_{i_1 i_2} K^{(\ell)}_{\alpha_1\alpha_2} + O(1/n_{\ell-1})\, , \\
    &\langle \phi^{(\ell)}_{i_1,\alpha_1} \phi^{(\ell)}_{i_2,\alpha_2}
      \phi^{(\ell)}_{i_3,\alpha_3} \phi^{(\ell)}_{i_4,\alpha_4}\rangle_c
      = O(1/n_{\ell-1})\, ,
\end{align}
where\footnote{
    The notation here refers to the matrix element $(\alpha_1,\alpha_2)$ of the inverse matrix of $\gamma^{(\ell)}$, and {\em not}\ to the inverse
    of the matrix element $\gamma^{(\ell)}_{\alpha_1\alpha_2}$.
}
\begin{equation}
    \label{eq:DefineKmat}
    K^{(\ell)}_{\alpha_1\alpha_2} = \left(\gamma^{(\ell)}\right)^{-1}_{\alpha_1\alpha_2}\, .
\end{equation}
The ``evolution'' of the couplings as we go deep in the NN, \ie, the dependence of the couplings on
$\ell$, is governed by Renormalization Group (RG) equations, which preserve the power counting in
powers of $1/n_{\ell}$. At leading order,
\begin{align}
    K^{(\ell+1)}_{\alpha_1\alpha_2} &=
      \left.
      C_b^{(\ell+1)} + C_w^{(\ell+1)} \frac{n_\ell}{n_\ell+n_{\ell-1}}\frac{1}{n_\ell}
      \langle \vec{\rho}^{\,(\ell)}_{\alpha_1} \cdot
      \vec{\rho}^{\,(\ell)}_{\alpha_2} \rangle
      \right|_{O(1)} \\
      \label{eq:RecursionForK}
      &= C_b^{(\ell+1)} + C_w^{(\ell+1)} \frac{n_\ell}{n_\ell+n_{\ell-1}}\frac{1}{n_\ell}
      \langle \vec{\rho}^{\,(\ell)}_{\alpha_1} \cdot
      \vec{\rho}^{\,(\ell)}_{\alpha_2} \rangle_{K^{(\ell)}}\, ,
\end{align}
where
\begin{align*}
    \frac{1}{n_\ell}
      \langle \vec{\rho}^{\,(\ell)}_{\alpha_1} \cdot
      \vec{\rho}^{\,(\ell)}_{\alpha_2} \rangle_{K^{(\ell)}} =
    \int \mathcal{D}\phi\,
      \frac{e^{-\frac12 \left(K^{(\ell)}\right)^{-1}_{\beta_1\beta_2}
        \phi_{\beta_1} \phi_{\beta_2}}}
        {\left|2\pi K^{(\ell)}\right|^{1/2}}\,
        \rho(\phi_{\alpha_1}) \rho(\phi_{\alpha_2})\, , 
\end{align*}
and
\begin{align}
    \label{eq:FunctIntDef}
    \mathcal{D}\phi = \prod_{\alpha=1}^{\ngrid} d\phi_\alpha\, .
\end{align}
Note that the integration variables in Eq.~\eqref{eq:FunctIntDef} do not have a
neuron index and the integrals are $\ngrid$ dimensional integrals.
Eq.~\eqref{eq:RecursionForK} is iterated for the NNPDF architecture, yielding
$K^{(\ell)}$ for arbitrary $\ell$, \ie, the covariance at initialisation for
various depths. These are compared with the empirical covariance computed from
an ensemble replicas in Fig.~\ref{fig:KRecursion} for the first two hidden layers
and the output layer. Furthermore, the relative difference between the empirical
covariance and the theoretical prediction is shown in Fig.~\ref{fig:delta_K}. In
order to reduce the bootstrap errors in the empirical covariance, an ensemble
with $\nreps=1000$ has been used for these figures. The agreement between the
theoretical prediction and the empirical computation is excellent, confirming
the validity of the large-network expansion even for networks of moderate size,
as those used in the NNPDF fits.
\begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.58]{section_2/K1_correlations.pdf}
    \includegraphics[scale=0.58]{section_2/K2_correlations.pdf}
    \includegraphics[scale=0.58]{section_2/K3_correlations.pdf}
    \caption{The empirical (left) and analytical (right) covariance matrices of
    the first, second and output layers of the NNPDF architecture (top to
    bottom). The covariance in the left panel is computed ``bootstrapping'' over
    an ensemble of replicas, initialised using the Glorot normal distribution.
    The covariance in the right panel is obtained by solving
    Eq.~\eqref{eq:RecursionForK} numerically. In order to reduce the bootstrap
    errors in the empirical covariance, an ensemble of 1000 replicas has been
    used for this figure.}
    \label{fig:KRecursion}
\end{figure}
\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.86\textwidth]{section_2/delta_K.pdf}
    \caption{Relative difference between the empirical kernel,
    computed from an ensemble of networks at initialisation, and the recursive
    kernel obtained by iterating Eq.~\eqref{eq:RecursionForK} for the three layers
    of the NNPDF architecture. An ensemble of 1000 replicas has been used to
    reduce the bootstrap errors in the empirical covariance.}
    \label{fig:delta_K}
\end{figure}


As a consequence of the symmetry of the probability distribution, the mean value
of the fields at initialisation needs to vanish, while their variance at each
point $x_\alpha$ is given by the diagonal matrix elements of $K^{(\ell)}$. In
Fig.~\ref{fig:OutputDist}, the expected distribution is compared against the
empirical distribution of output fields for a selected value of $x$, using two
ensembles of replicas with $\nreps=100$ and $\nreps=1000$, respectively.
Inspecting the figures, we conclude that the recursion formula,
Eq.~\eqref{eq:RecursionForK}, accurately reproduces the output distribution of
the NNPDF networks at initialisation, provided that a sufficiently large
ensemble of replicas is used to sample the distribution. Finally,
Fig.~\ref{fig:prior} shows the mean and variance of the output at initialisation
across all values of $x$ for an ensemble of $\nreps=100$ neural networks
generated using the NNPDF architecture. We compare two cases: linear input
$f(x)$ and scaled input $f(x, \log x)$ as defined in
Eq.~\eqref{eq:InitLayerPhi}. The central value is computed according to
Eq.~\eqref{eq:ReplicaEnsemble},
\begin{align}
    \label{eq:MeanValAtInit}
    \bar{f}_{i\alpha} = \bar{f}_{i}(x_\alpha) = \frac{1}{\nreps} \sum_{k=1}^{\nreps} f^{(k)}_i(x_\alpha)\, ,
\end{align}
and the variance $\sigma^2_{i\alpha}$ is computed using the same formula with
\begin{align}
    \label{eq:VarAtInit}
    O(f) = \frac{\nreps}{\nreps-1} \left(f_i(x_\alpha) - \bar{f}_{i}(x_\alpha)\right)^2\, .
\end{align}
As is clear from the figure, the choice of input scaling has a significant impact
of the prior uncertainty, especially in the small-$x$ region. In the following,
we neglect this effect and focus on the case of linear input $f(x)$.

\begin{figure}
  \centering
  \includegraphics[width=0.95\textwidth]{section_2/output_distribution_100.pdf}
  \includegraphics[width=0.95\textwidth]{section_2/output_distribution_1000.pdf}
  \caption{Sampled distribution of the output $xT_3$ at $x=0.0065$ for two
  different ensemble sizes, $\nreps=100$ (top) and $\nreps=1000$ (bottom). Each
  column shows the distribution for a different network architecture, the latter
  displayed in the top left corner of each panel. The red line the represents
  the predicted Gaussian distribution as dictated by the kernel recursion
  formula in Eq.~\eqref{eq:RecursionForK}.
  \label{fig:OutputDist}}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.45\textwidth]{section_2/prior.pdf}
    \includegraphics[width=0.45\textwidth]{section_2/prior_log.pdf}
    \caption{The output of the ensemble of neural networks at initialisation
    using the NNPDF architecture in linear (left) and logarithm (right) scale.
    We compare the case of linear input $f(x)$ (blue) and the case of
    scaled input $f(x, \log x)$ (orange). The solid lines represent
    the mean value computed over an ensemble of 100 replicas, while the shaded
    bands represent the one-sigma uncertainty computed as the variance over the
    same ensemble. In the figure, we show $xT_3$ as used in the following
    sections.}
    \label{fig:prior}
\end{figure}


\FloatBarrier
