\section{On Neural Networks and Inverse Problems}
\label{sec:Init}

In this section, we discuss some non-trivial properties of neural networks at
initialization. Then, we present the framework adopted in this work and
introduce the notation used in the rest of the paper.

\subsection{The Inverse Problem for $xT_3$}
\label{subsec:inverse_problem}

The extraction of PDFs from experimental data is a classic example of an inverse
problem, namely the reconstruction of a function $f(x)$ from a finite set of
data points $Y_I$, where the index $I=1, \ldots, \ndat$.\footnote{When omitting
the data index $I$, we will always assume $Y \in \mathbb{R}^{\ndat}$.} In
particular, for this study, we will focus on DIS data, which depend linearly on
the function $f(x)$. The theoretical prediction for the data point $Y_I$ is
given by
\begin{equation}
    \label{eq:TheoryPred}
    T_I[f] = \sum_{i=1}^{\nflav} \int dx\, C_{Ii}(x) f_{i}(x)\, ,
\end{equation}
where $C_{Ii}(x)$ is a coefficient function, known to some given order in
perturbation theory, $i$ labels the parton flavour, and $f_i(x)$ is the PDF (or
set of PDFs) that we want to determine.

In this work, we will 

Attempting to determine a function $f$ in an infinite dimensional space of
solutions with a finite set of data is inherently ill-posed. The solution will
inevitably depend on assumptions and prior knowledge, conscious or not,
introduced to regularise the problem. The solution of the inverse problem
can be tackled either by using non-parametric methods, as recently proposed
in Refs.~\cite{DelDebbio:2021whr,Candido:2024hjt}, or, more commonly in the
filed of PDF determination, by introducing a parameterization for the function
$f$. Many state-of-the-art PDF determinations belong to the latter, and
they differ, among other things, in the choice of the parameterization and the
fitting methodology.

As mentioned, understanding the difference between the various approaches is
crucial for precision physics. For instance, different parametrisations for
can lead to a bias in the the space of te reconstructed solution. Furthermore,
the interplay between the parameterization and the fitting methodology determines
how the uncertainties on the data propagate to the uncertainties on the fitted
solution. Understanding the bias and variance of the fitted PDF is therefore a
major challenge for precision physics.

Following the ideas highlighted in
Refs.~\cite{DelDebbio:2021whr,Candido:2024hjt}, the solution of the inverse
problem is conveniently phrased in a Bayesian framework. The functions $f_i$ are
promoted to stochastic processes; for any grid of points $x_{\alpha}$,
$\alpha=1, \ldots, \ngrid$, the vector $f_{i\alpha}=f_{i}(x_{\alpha})$ is a
vector of $\nflav\times\ngrid$ stochastic variables, for which we introduce a
prior distribution $p(f)$.~\footnote{Following the same convention used for the
data, when omitting the grid index $\alpha$, and/or the flavor index $i$, we
will always refer to a vector $f \in \mathbb{R}^{\nflav\times\ngrid}$.}

Any fitting procedure is interpreted as a recipe that yields the posterior
distribution $\tilde{p}(f)$.

In this study, probability distributions are represented by ensembles of i.i.d.
replicas. So, for instance, the prior distribution $p(f)$ is described by an
ensemble
\begin{equation}
    \label{eq:RepDef}
    \left\{f^{(k)} \in \mathbb{R}^{\nflav\times\ngrid}; k=1, \ldots, \nreps\right\}\, ,
\end{equation}
drawn from the distribution $p$, so that
\begin{equation}
    \label{eq:ReplicaEnsemble}
    \mathbb{E}_{p}[O(f)] = \frac{1}{\nreps} \sum_{k=1}^{\nreps} O(f^{(k)})\, ,
\end{equation}
for any observable $O$ that is built from the PDFs.

The prior distribution $p(f)$ is defined by initializing a set of replicas using
a Glorot normal initializer~\cite{glorot2010understanding}. The result of this
initialization is discussed below in Sec.~\ref{sec:Init}. For each replica, a
new set of data $Y^{(k)}$ is generated from an $\ndat$ dimensional Gaussian
distribution centred at the experimental central value $Y$, with the covariance
given by the experimental covariance matrix $C_Y$,
\begin{equation}
    \label{eq:ExpReplicaDistr}
    Y^{(k)} \sim \mathcal{N}\left(Y, C_Y\right)\, .
\end{equation}
Each replica $f^{(k)}$ is trained on its corresponding data set $Y^{(k)}$. We
denote the replicas at training time $t$, $f^{(k)}_{t} \in
\mathbb{R}^{\nflav\times\ngrid}$. Stopping the training at time $T$, the
posterior probability distribution is represented by the set of replicas
$\left\{f^{(k)}_{T}\in \mathbb{R}^{\nflav\times\ngrid}; k=1, \ldots,
\nreps\right\}$, so that averages over the posterior distribution are computed
as
\begin{equation}
    \label{eq:PostEnsemble}
    \mathbb{E}_{\tilde{p}}[O(f)] = \frac{1}{\nreps} \sum_{k=1}^{\nreps}
        O\left(f^{(k)}_{T}\right)\, .
\end{equation}
All knowledge about the solution of the inverse problem, $f$, is encoded in the
posterior $\tilde{p}$ and is expressed as expectation values of observables $O$
using Eq.~\eqref{eq:PostEnsemble}.

\subsection{Neural Networks at Initialisation}
% \label{subsec:NNinit}

When initializing a neural network, the weights and biases -- which we denote collectively as the {\em parameters}\ 
of the network -- are drawn from some probability distribution. In 
the NNPDF formalism, the set of network parameters at initialization for each replica is an instance 
of i.i.d. stochastic variables. The probability distribution
of the network parameters induces a probability distribution for the output of the neural networks at inizialization. 
It is well known that the probability distribution of these outputs becomes approximately gaussian when the size of
the hidden layers is increased. We call this limit the {\em large-network} limit. 
In this section, we review some basic results for the large-network limit at inizialization, and 
compare these theoretical 
predictions with the actual results obtained in numerical experiments performed with the typical networks 
used by the NNPDF collaboration. 
This preliminary study of the properties of networks at initialization also allows us to introduce the 
notation used for the networks in the rest of the paper. 

As detailed in Ref.~\cite{NNPDF:2021njg}, the NNs used for the NNPDF fit have a 2-25-20-8 architecture,
a $\tanh$ activation function,
and are initialized using a Glorot normal distribution~\cite{glorot2010understanding}. The preactivation
function of a neuron is denoted as $\phi^{(\ell)}_{i,\alpha} = \phi^{(\ell)}_i(x_\alpha)$, where $\ell$
denotes the layer of the neuron, $i$ identifies the neuron within the layer\footnote{We refer
to $i$ as the {\em neuron}\ index.}, and $x_{\alpha}$ is a point in the interval $[0,1]$.
A grid of $\ngrid=50$ points is used to compute observables in the NNPDF formalism and in this work 
we focus on the vakue of $f$ at those values of $x_\alpha$. For completeness, we list the values of $x_\alpha$ in
Tab.~\ref{tab:Xvals}.

\begin{table}[ht]
    \centering
    \input{tabs/grid.tex}
    \caption{Values of $x_\alpha$ used in the NNPDF grids for the computation of
    observables. The points are equally spaced on a logarithmic scale
    for $\alpha = 1, \ldots, XXX$, and linearly spacing for $\alpha > XXX$.
    \ac{Maybe we need to rethink the layout of this table...}
    \label{tab:Xvals}}
\end{table}

The output of the neuron identified by the pair $(\ell,i)$ is
$\rho^{(\ell)}_{i\alpha} = \tanh\left(\phi^{(\ell)}_{i\alpha}\right)$.
The parameters of the NN are the weights $w^{(\ell)_{ij}}$ and the biases $b^{(\ell)}_i$, which are
collectively denoted as $\theta_\mu$, where $\mu = 1, \ldots, P$ and the total number of parameters
is
\begin{equation}
    \label{eq:TotPar}
    P = \sum_{\ell=1}^{L} \left(n_{\ell} n_{\ell-1} + n_\ell\right)\, .
\end{equation}
The preactivation function in layer $(\ell+1)$ is a weighted average of the outputs of the neurons on 
the previous layer, namely
\begin{align}
    \label{eq:RecursionNN}
    \phi^{(\ell+1)}_{i\alpha} = \sum_{j=1}^{n_\ell} w^{(\ell+1)}_{ij} \rho^{(\ell)}_{i\alpha} + b^{(\ell+1)}_{i}\, .
\end{align}
The PDFs in the
so-called evolution basis are parametrized by the preactivation functions of the output layer $L$,
$x_\alpha f_i(x_\alpha)=A_i \phi^{(L)}_{i,\alpha}$, where $i=1, \ldots, 8$ labels the flavors.
\footnote{For simplicity, we ignore the preprocessing function $x^{-\alpha_i} (1-x)^{\beta_i}$ that
is currently used in the NNPDF fits. While the preprocessing may be useful in speeding the training
it does not affect the current discussion.}
The input layer is identified by $\ell=0$ and the activation
function for that specific layer is the identity, so that
\begin{equation}
    \label{eq:InitLayerPhi}
    \rho^{(0)}_{i,\alpha} = \phi^{(0)}_{i,\alpha} = x_{i,\alpha} =
    \begin{cases}
        x_\alpha\, , \quad &\text{for}\ i=1\, ;\\
        \log\left(x_\alpha\right)\, , \quad &\text{for}\ i=2\, .
    \end{cases}
\end{equation}
In the following we refer to the preactivation functions as {\em fields}.

The Glorot normal initialiser draws each weight and bias of the NN from independent Gaussian
distributions, denoted $p_w$ and $p_b$ respectively, centred at zero and with variances
rescaled by the number of nodes in adjacent layers,
\begin{equation}
    \label{eq:RescaledGlorotVariances}
    \frac{C^{(\ell)}_{w}}{\sqrt{n_{\ell-1} + n_{\ell}}}\, ,
    \quad \frac{C^{(\ell)}_{b}}{\sqrt{n_{\ell-1} + n_{\ell}}}\, .
\end{equation}
Following the NNPDF prescription, we have $C^{(\ell)_{w}=C^(\ell)}_{b}=1$. 
The probability distribution of the NN parameters induces a probability distribution for the
preactivations,
\begin{align}
    \label{eq:PreactAtInit}
    p\left(\phi^{(\ell)}\right)
      &= \int \mathcal{D}w\, p_w(w)\,
        \mathcal{D}b\, p_b(b)\, \prod_{i,\alpha}
        \delta\left(
          \phi^{(\ell)}_{i\alpha} - \sum_{j} w^{(\ell)}_{ij}
          \rho\left(\phi^{(\ell-1)}_{j\alpha}\right)
          - b^{(\ell)}_i
          \right)\, .
\end{align}
Note that, here and in what follows, $p(\phi^{(\ell)})$ denotes the joint probability for all the
$n_{\ell}\times\ngrid$ components of $\phi^{(\ell)}$,
\begin{align}
    \label{eq:ExplIndices}
    p\left(\phi^{(\ell)}\right) = p\left(\phi^{(\ell)}_{1,\alpha_1}, \phi^{(\ell)}_{2,\alpha_1}, \ldots,
        \phi^{(\ell)}_{n_\ell,\alpha_1}, \phi^{(\ell)}_{1,\alpha_2}, \ldots, \phi^{(\ell)}_{n_\ell,\alpha_2},
        \ldots,
        \phi^{(\ell)}_{n_\ell,\ngrid}\right)\, .
\end{align}
This duality between parameter-space and function-space provides a powerful framework to study
the behaviour of an ensemble of NNs, and in particular the symmetry properties of the distribution
$p(\phi^{(\ell)})$, see \eg~\cite{Maiti:2021fpy}. Working in parameter space, \ie\ computing the
expectation values of correlators of fields as integrals over the NN parameter, one can readily
show that
\begin{align}
    \label{eq:NeurRotInv}
    \mathbb{E}\left[
        R_{i_1j_1} \phi^{(n_\ell)}_{j_1 \alpha_1} \ldots
        R_{i_nj_n} \phi^{(n_\ell)}_{j_n \alpha_n}
    \right] =
    \mathbb{E}\left[
        \phi^{(n_\ell)}_{i_1 \alpha_1} \ldots
        \phi^{(n_\ell)}_{i_n \alpha_n}
    \right]\, ,
\end{align}
where $R$ is an orthogonal matrix in $\text{SO}(n_{\ell})$. Eq.\eqref{eq:NeurRotInv} implies
that the probability distribution in Eq.~\eqref{eq:PreactAtInit} is also invariant under rotations,
and therefore it can only be a function of $\text{SO}(n_{\ell})$ invariants. Therefore
\begin{align}
    \label{eq:PriorAction}
    p\left(\phi^{(n_\ell)}\right) =
        \frac{1}{Z^{(\ell)}} \exp\left(-S\left[\phi^{(\ell)}_{\alpha_1}
            \cdot \phi^{(\ell)}_{\alpha_2}\right]\right)\, ,
\end{align}
where
\begin{align}
    \label{eq:PhiInvariant}
    \phi^{(\ell)}_{\alpha_1}
            \cdot \phi^{(\ell)}_{\alpha_2} =
    \sum_{i=1}^{n_\ell} \phi^{(\ell)}_{i \alpha_1} \phi^{(\ell)}_{i \alpha_2}\, .
\end{align}
The action can be expanded in powers of the invariant bilinear,
\begin{align}
    \label{eq:ExpandAction}
    S\left[\phi^{(\ell)}_{\alpha_1}
            \cdot \phi^{(\ell)}_{\alpha_2}\right] =
        \frac12 \gamma^{(\ell)}_{\alpha_1\alpha_2}
            \phi^{(\ell)}_{\alpha_1} \cdot \phi^{(\ell)}_{\alpha_2} +
            \frac{1}{8 n_{\ell-1}} \gamma^{(\ell)}_{\alpha_1\alpha_2,\alpha_3\alpha_4}
            \phi^{(\ell)}_{\alpha_1} \cdot \phi^{(\ell)}_{\alpha_2} \,
            \phi^{(\ell)}_{\alpha_3} \cdot \phi^{(\ell)}_{\alpha_4} + O(1/n_{\ell-1}^2)\, ,
\end{align}
so that the probability distribution is fully determined by the couplings 
$\gamma^{(\ell)}$.\footnote{
    We have denoted {\em all}\ couplings by $\gamma^{{(\ell)}}$. Different couplings 
    are indentified by the number of indices, so that $\gamma^{(\ell)}_{\alpha_1\alpha_2}$ 
    is a two-point coupling, $\gamma^{(\ell)}_{\alpha_1\alpha_2,\alpha_3\alpha_4}$ is a four-point 
    coupling, etc. 
} 
In
Eq.~\eqref{eq:ExpandAction}, we have factored out inverse powers of $n_\ell$ for each coupling.
With this convention, and with the scaling of the parameters variances in
Eq.~\eqref{eq:RescaledGlorotVariances}, the couplings in the action are all $O(1)$
in the limit where $n_\ell\to\infty$.
As a consequence, the probability distribution at initialization is a multidimensional Gaussian at
leading order in $1/n_\ell$, with quartic corrections that are $O(1/n_\ell)$, while higher powers
of the invariant bilinear are suppressed by higher powers of the width of the layer. This power counting
defines an effective field theory, where deviations from Gaussianity can be computed in perturbation
theory to any given order in $1/n_\ell$, see \eg\ Ref.~\cite{Roberts:2021fes} for a detailed
presentation of these ideas. While the actual calculations become rapidly cumbersome, the
conceptual framework is straightforward.

At leading order, the second and fourth cumulant are respectively
\begin{align}
    &\langle \phi^{(\ell)}_{i_1,\alpha_1} \phi^{(\ell)}_{i_2,\alpha_2}\rangle
      = \delta_{i_1 i_2} K^{(\ell)}_{\alpha_1\alpha_2} + O(1/n_{\ell-1})\, , \\
    &\langle \phi^{(\ell)}_{i_1,\alpha_1} \phi^{(\ell)}_{i_2,\alpha_2}
      \phi^{(\ell)}_{i_3,\alpha_3} \phi^{(\ell)}_{i_4,\alpha_4}\rangle_c
      = O(1/n_{\ell-1})\, ,
\end{align}
where
\begin{equation}
    \label{eq:DefineKmat}
    K^{(\ell)}_{\alpha_1\alpha_2} = \left(\gamma^{(\ell)}\right)^{-1}_{\alpha_1\alpha_2}\, .
\end{equation}
The ``evolution'' of the couplings as we go deep in the NN, \ie\ the dependence of the couplings on
$\ell$, is governed by Renormalization Group (RG) equations, which preserve the power counting in
powers of $1/n_{\ell}$. At leading order,
\begin{align}
    K^{(\ell+1)}_{\alpha_1\alpha_2} &=
      \left.
      C_b^{(\ell+1)} + C_w^{(\ell+1)} \frac{1}{n_\ell}
      \langle \vec{\rho}^{\,(\ell)}_{\alpha_1} \cdot
      \vec{\rho}^{\,(\ell)}_{\alpha_2} \rangle
      \right|_{O(1)} \\
      \label{eq:RecursionForK}
      &= C_b^{(\ell+1)} + C_w^{(\ell+1)} \frac{1}{n_\ell}
      \langle \vec{\rho}^{\,(\ell)}_{\alpha_1} \cdot
      \vec{\rho}^{\,(\ell)}_{\alpha_2} \rangle_{K^{(\ell)}}\, ,
\end{align}
where
\begin{align*}
    \frac{1}{n_\ell}
      \langle \vec{\rho}^{\,(\ell)}_{\alpha_1} \cdot
      \vec{\rho}^{\,(\ell)}_{\alpha_2} \rangle_{K^{(\ell)}} =
    \int \prod_{\alpha}d\phi_\alpha\,
      \frac{e^{-\frac12 \left(K^{(\ell)}\right)^{-1}_{\beta_1\beta_2}
        \phi_{\beta_1} \phi_{\beta_2}}}
        {\left|2\pi K^{(\ell)}\right|^{1/2}}\,
        \rho(\phi_{\alpha_1}) \rho(\phi_{\alpha_2})\, .
\end{align*}
Eq.~\eqref{eq:RecursionForK} can be solved for the NNPDF architecture leading to the
covariance matrices for the output of the NNs displayed in
Figs.~\ref{Fig:KRecursionOne} and~\ref{Fig:KRecursionTwo}.
\begin{figure}[t!]
    \centering
    \includegraphics[scale=0.4]{figs/K1_correlations.pdf}
    \caption{The empirical (left) and analytical (right) covariance matrices $K^{(1)}$ of the first layer
    of the NNPDF architecture. The covariance in the left panel is computed ``bootstrapping'' over an
    ensemble of 100 replicas, initialised using the Glorot normal distribution. The covariance in the right
    panel is obtained by solving Eq.~\eqref{eq:RecursionForK} numerically.
    \label{Fig:KRecursionOne}
    }
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[scale=0.4]{figs/K2_correlations.pdf}
    \includegraphics[scale=0.4]{figs/K3_correlations.pdf}
    \caption{Same as Fig.~\ref{Fig:KRecursionOne}, but for the second (top) and third (bottom) layers of the
    NNPDF architecture.
    \ac{Here the four indices of the covariance $K_{i_1i_2, \alpha_1\alpha_2}$ are flattened into
    two indices for the sake of graphical representation. Maybe we should group the labels into groups of
    $N_{\rm grid}$ ticks on the axes.}
    \label{Fig:KRecursionTwo}
    }
\end{figure}

As a consequence of the symmetry of the probability distribution, the mean value of the fields at
initialization needs to vanish, while their variance at each point $x_\alpha$ is given by the
diagonal matrix elements of $K^{(\ell)}$. The central value and the variance of the
parametrized singlet ($\Sigma$) and gluon ($g$) at initialization are shown in
Fig.~\ref{fig:SingletGluonInit} for an ensemble of $\nreps=100$. The central value is computed as
discussed above in Eq.~\eqref{eq:ReplicaEnsemble},
\begin{align}
    \label{eq:MeanValAtInit}
    \bar{f}_{i\alpha} = \bar{f}_{i}(x_\alpha) = \frac{1}{\nreps} \sum_{k=1}^{\nreps} f^{(k)}_i(x_\alpha)\, ,
\end{align}
and the variance $\sigma^2_{i\alpha}$ is computed using the same formula with
\begin{align}
    \label{eq:VarAtInit}
    O(f) = \frac{\nreps}{\nreps-1} \left(f_i(x_\alpha) - \bar{f}_{i}(x_\alpha)\right)^2\, .
\end{align}

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{plots/UoECentredLogo282v1160215.png}
    \caption{\ldd{Plot of replicas at init}}        
    \label{fig:SingletGluonInit} 
\end{figure}

\ldd{List of plots that we still need to do:
comparison analytical vs empirical, 
comparison analytical vs Gaussian processes}

% \paragraph{Dependence on the architecture.}
Having analytical expressions for the variance at initialization allows us to investigate the
impact of the NN architecture on the prior that is imposed on the PDFs. Iterating
Eq.~\eqref{eq:RecursionForK} yields the covariance at initialization for various depths.
Do we get something interesting? worth mentioning?

\ldd{We should also look at the recursion relations with other activations. Again, check whether we
get something interesting...}

\FloatBarrier
