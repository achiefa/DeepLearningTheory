\section{Training Dynamics and the Neural Tangent Kernel}
\label{sec:Training}

Having defined the physical context of this work and established some properties
of the neural network at initialisation, we now turn to the optimisation
process. In the context of machine learning, specifically when dealing with
neural networks, optimisation is an iterative algorithm that updates the
parameters of the network in order to minimise a figure of merit defined
appropriately. Due to the large number of parameters that characterise a neural
network, the figure of merit (also known as \textit{error function},
\textit{loss function}, or simply \textit{loss}) is a non-convex
high-dimensional function, posing a challenge in the minimisation task. In
addition, in order to avoid \textit{over-} and \textit{under-learning}, these
training algorithms are paired with the so-called \textit{stopping criterion},
which specifies the optimal condition to end the training process.

In practice, this task is tackled by using gradient methods where the direction
towards the minimum is defined by the gradient of the loss function. These
methods are usually improved by including, for instance, stochasticity and
information on previous iterations. A detailed overview of these extended
gradient methods is beyond the scope of this work. In the context of PDF
determinations, the NNPDF collaboration makes intensive use of these tools and
the reader is encouraged to refer to Ref.~\cite{NNPDF:2021njg} for an extensive
discussion.

Our main aim in this paper is understanding the dynamics driving the training
process. Indeed, while these algorithms have achieved remarkable empirical
success, a theoretical understanding of the optimization process remains
elusive. Therefore, we work in a simplified setting where we consider the
simplest gradient method, \textit{i.e.} Gradient Descent (GD), and and data that
depend linearly on the unknwon PDFs, as shown in Eq.~\eqref{eq:TheoryPred}.
Furthermore, we do not split the dataset between training and
validation\footnote{We are aware that such a framework has limited applicability
in the context of PDF determination, and we are far from assuming this as an
optimal choice. Again, the focus remain on the methodological aspects rather
than the underlying physics.} The generalization to other minimizers and
non-linear data is left to future investigations, but is expected to yield
qualitatively similar results. Finally, we remark that the results in this
section, although obtained having in mind neural networks, apply to any generic
parametrization of the unknown function, whether it is a polynomial or a
kernel~\cite{Costantini:2025wxp}.

\subsection{Training in Functional Space}
\label{sec:GradFlow}

Gradient descent is described as a continuous flow of the parameters $\theta$ in
training time $t$ along the negative gradient of the loss function
$\mathcal{L}$. Following from the parameter-space/function-space duality
introduced in Sec.~\ref{sec:Init}, we aim at rephrasing the optimisation process
of GD in the space of the network output $f$. To ease the mathematical
tractability, we employ the continuous version of GD, which has been
shown~\cite{barrett2022igr} to match the discretised version as long as the
learning rate is small enough. The continuous Gradient Flow (GF) is then given
by
\begin{align}
    \label{eq:GradientFlowDef}
    \ddt &\theta_{t,\mu} = -\nabla_\mu \mathcal{L}_t\, ,
\end{align}
where $\theta_{t,\mu}$ and $\mathcal{L}_t$ identify respectively the parameter
and the loss function at training time $t$. We focus here on quadratic loss
functions that are obtained as the negative logarithm of Gaussian data
distributions around their theoretical predictions,
\begin{align}
    \label{eq:QuadLoss}
    \mathcal{L}_t = \frac12 \left(Y - T[f_t]\right)^T C_Y^{-1} \left(Y - T[f_t]\right)\, ,
\end{align}
where $f_t$ is the output of the network at training time $t$, which follows
from the time-dependence of the internal parameters. Here $C_Y$ is the
covariance of the data, which includes statistical and systematic errors given
by the experiments and also any theoretical error, like \eg\ missing higher
orders in the theoretical predictions. Indices that are summed over are
suppressed to improve the clarity of the equations. Note that the loss function
at training time $t$ is computed using the theoretical prediction $T[f_t]$, \ie\
the result of Eq.~\eqref{eq:TheoryPred} computed using the fields at training
time $t$. For a quadratic loss, the gradient is
\begin{align}
    \nabla_\mu \mathcal{L}_t = - \left(\nabla_\mu f_t\right)^T \left(\frac{\partial T}{\partial f}\right)_t
      C_Y^{-1} \epsilon_t\, ,
\end{align}
where, writing explicitly the data index,
\begin{align}
    \label{eq:EpsDef}
    \epsilon_{t,I} = Y_I - T_I[f_t]\, , \quad I=1, \ldots, \ndat\, .
\end{align}
For the specific case of a quadratic loss function, the gradient is proportional
to $\epsilon_t$, which is the difference between the theoretical prediction and
the data at training time $t$. If at some point during the training the
theoretical predictions reproduce all the data, the training process ends. A
further simplification is obtained in the case of data that depend linearly on
the unknown function $f$. In the specific case of NNPDF fits, the integrals in
Eq.~\eqref{eq:TheoryPred} are approximated by a Riemann sum over the grid of $x$
points,
\begin{align}
    \label{eq:FKTabDef}
    T_I[f] \approx \sum_{i=1}^{\nflav}\sum_{\alpha=1}^{\ngrid} \FKtab_{Ii\alpha} f_{i\alpha}\, ,
\end{align}
and hence
\begin{align}
    \label{eq:dTbydf}
    \left(\frac{\partial T_I}{\partial f_{i\alpha}}\right)_t =
        \FKtab_{Ii\alpha}\, ,
\end{align}
which is independent of $t$. With simple algebraic steps, the flow of parameters
$\theta$ can be translated into a flow for the fields,
\begin{align}
    \label{eq:NTKFlow}
    \ddt &f_{t,i_1\alpha_1} = (\nabla_\mu f_{t,i_1\alpha_1}) \ddt \theta_\mu =
      \Theta_{t,i_1\alpha_1i_2\alpha_2}
      \FKtabT_{i_2\alpha_2I} \left(C_Y^{-1}\right)_{IJ} \epsilon_{t,J}\, ,
\end{align}
where we have defined the Neural Tangent Kernel~\cite{jacot2018neural}
\begin{align}
    \label{eq:NTKDef}
    \Theta_{t,i_1\alpha_1i_2\alpha_2} = \sum_\mu
    \nabla_\mu f_{t,i_1\alpha_1} \nabla_\mu f_{t,i_2\alpha_2}\, .
\end{align}

In order to facilitate the discussion in Sec.~\ref{sec:Lazy},
Eq.~\eqref{eq:NTKFlow} can be rewritten in a more compact form. We first omit
the indices such that, for instance,
\begin{align}
  \left(\frac{\partial T}{\partial f}\right)_t = \FKtab\, , \quad
  \Theta_t = \left(\nabla_\mu f_t\right) \left(\nabla_\mu f_t\right)^T\, .
  \label{eq:dTdfForLinearObs}
\end{align}
Then, using the definition of the error in Eq.~\eqref{eq:EpsDef}, we can rewrite
Eq.~\eqref{eq:NTKFlow} as follows
\begin{align}
    \label{eq:FlowEquationNoIndices}
    \ddt f_t = -\Theta M f_t + b\, ,
\end{align}
where
\begin{align}
    M &= \FKtabT C_Y^{-1} \FKtab\, , \quad b = \Theta \FKtabT C_Y^{-1} Y\, .
\end{align}
Here $M$ is a positive-semidefinite matrix that depends only on the data and the
theoretical predictions, while $b$ is a vector that depends also on the data.

Before moving to the next subsection, a few comments are due. First, although
derived in the context of neural networks, these equations do not refer to a
specific parameterization. Indeed, these remain valid even when an explicit
functional form to parametrize the PDFs is chosen, as \eg in
Refs.~\cite{Bailey:2020ooq,Hou:2019efy,Costantini:2025wxp}. Second, it is
interesting to observe that the flow equation,
Eq.~\eqref{eq:FlowEquationNoIndices}, depends on two matrices, $\Theta$ and $M$.
The former encodes the model dependence, while the latter brings physical
information. The interplay between these two matrices is crucial for
understanding the training dynamics, as it will be discussed in
Sec.~\ref{sec:NTKAlign}. Finally, the NTK derived in Eq.~\ref{eq:NTKDef} is
inherently time-dependent in a complex way, which precludes any attempt in
integrating Eq.~\ref{eq:FlowEquationNoIndices} analytically. We will come back
to this point in Sec.~\ref{sec:Lazy}, and we now turn to discussing the
properties of the NTK during training.

\subsection{Inside the Training Dynamics: an NTK perspective}

The NTK introduced above provides a powerful framework for understanding neural
network dynamics during training. Originally developed by Jacot et
al.~\cite{jacot2018neural} to analyse infinite-width feed-forward networks, the
NTK theory has since been extended to diverse architectures including
convolutional networks~\cite{arora2019exact} and recurrent
networks~\cite{alemohammad2021recurrent}. This theoretical framework has proven
invaluable for characterizing learning dynamics and generalization properties
across various network designs.

From Eqs.~\eqref{eq:NTKDef} and~\eqref{eq:FlowEquationNoIndices}, we observe
that the NTK encodes the dependence on the architecture of the network and
governs its training dynamics. The analysis of the NTK properties is thus
crucial for understanding the behaviour of the network during training. We first
discuss the properties of the NTK at initialisation, before moving to the
training phase, where we provide a detailed study of the NTK in the context of
the NNPDF methodology.

% ===================================
\begin{figure}[t!]
  \centering
  \includegraphics[width=0.90\textwidth]{figs/section_3/ntk_initialization_with_uncertainty.pdf}
  \caption{Frobenius norm of the NTK at initialisation, $\lVert \Theta_0
  \rVert$, in function of the width of the network. On the left, the central
  values and uncertainty bands are obtained as the mean and one-sigma deviation
  of the ensemble of networks. The plot on the right shows the relative
  uncertainty.}
  \label{fig:NTKInit}
\end{figure}
% ===================================

\subsubsection{NTK at Initialization}
\label{sec:NTKAtInit}

Before training, the NTK is blind to data and depends, in addition to the
architecture, on the $x$-grid of input and on the architecture, as it can be
seen from Eq.~\eqref{eq:NTKDef}. The NTK is a function of the fields $f$, which
are stochastic variables described by their joint probability distribution as
discussed in Sect.~\ref{sec:Init}. Therefore the NTK is also a stochastic
variable, with its own probability distribution, which we represent as usual as
a set of replicas. 

It is argued in the literature that, in the large-width limit, the variance of
the NTK over the set of replicas tends to zero with the width of the hidden
layers (see, \textit{e.g.}, \cite{Roberts:2021fes}). In order to quantify the
variation of the NTK, we start by computing the Frobenius norm of the NTK over
an ensemble of networks for different architectures. For each architecture, we
consider the mean value and standard deviation of the norm as statistical
estimators of the variations of the NTK. The result is displayed in
Fig.~\ref{fig:NTKInit}. Even though the Frobenius norm is a coaarse indicator of
the variations of the NTK, the figure shows clearly that the variance of the
norm becomes smaller with the size of the network, which is consistent with the
theoretical expectation that the NTK should not fluctuate for infinite-width
networks\footnote{Note that, in addition to the scaling $\mathcal{O}(1/n)$
theoretically predicted for large networks, the uncertainty bands include
bootstrap errors due to the finite size of the ensemble. This amounts to $\sim
10\%$ of the total error quoted in the plots, as explicitly checked using
bootstrap over the ensemble.}.

% ===================================
\begin{figure}[t]
  \centering
  \includegraphics[width=0.45\textwidth]{figs/section_3/ntk_initialization_arch.pdf}
  \caption{Spectrum of the NTK at initialization for the architectures shown in
  Fig.~\ref{fig:NTKInit}. Error bands correspond to one-sigma uncertainties over
  the ensemble of networks.}
  \label{fig:NTKSpectrum}
\end{figure}
% ===================================

In order to get a more quantitative description of the NTK at initialization,
its spectrum is shown in Fig.~\ref{fig:NTKSpectrum} for four different
architectures. As debated in the literature~\cite{XXX}, the spectrum of the NTK
is heavily hierarchical, and only few eigenvalues are actually
non-zero\footnote{Note that, due to the large difference in magnitude of the
eigenvalues, the finite precision used in our codes introduces noise in the
decomposition, so that small eigenvalues should be effectively considered zero.
We discuss the cut-off tolerance later, when we discuss the training process in
more details.}. This means that only a small subset of active directions can
inform the network during training, as it will be discussed later. Note that, at
least at initialization, these observations do not depend on the architecture.
The eigenvalues in Fig.~\ref{fig:NTKSpectrum} are mostly independent of the size
of the network. There is a downward fluctuation of the third eigenvalue for the
largest architecture that we considered, but we do not have any evidence that
this drop is a physical feature of the system, rather than a fluctuation. the
variance of the set of eigenvalues over replicas decreases with increasing size,
as expected. 

% \FloatBarrier

\subsubsection{NTK During Training}
\label{sec:NTKDuringTraining}

% In the machine learning literature, it is argued that the NTK remains constant
% during training provided that the width of the network is large enough. Here
% we show that this is not the case, at least for the architectures used in the
% standard NNPDF methodology. 

Having established the properties of the NTK at initialisation, we now discuss
its behaviour during training. TO do so, we performed a fit of $T_3$ using the
NNPDF methodology with the dataset described in App.~\ref{app:dataset}. We
initialized an ensemble of $\nreps = 100$ replicas with identical architecture,
training each replica independently using GD optimization. As our focus here is
on NTK properties rather than physical predictions, we use generate three sets
of data with controlled noise characteristics -- L0, L1, and L2 -- following the
prescription described in App.~\ref{app:dataset}. Throughout the training
process, we track the evolution of the NTK to understand how the network's
effective dynamics change as it learns the target function.

\paragraph{Onset of Lazy Training} 

As a first estimator of the variation of the NTK, we show in the left panel of
Fig.~\ref{fig:NTKTime} the Frobenius norm of the variation during training,
normalized by the Frobenius norm of the NTK itself, 
\begin{equation}
\delta \Theta_t = \frac{\lVert \Theta_{t+1} - \Theta_t \rVert}{\lVert \Theta_t \rVert} \;,
\label{eq:DeltaNTK}
\end{equation}
for three different datasets, L0, L1, and L2. 

% ===================================
\begin{figure}[t]
  \centering
  \includegraphics[width=0.45\textwidth]{section_3/delta_ntk.pdf}
  \includegraphics[width=0.45\textwidth]{section_3/ntk_eigvals_single_plot_L0.pdf} 
  \caption{(Left) Relative variation of the NTK during training for L0, L1, and
  L2 data. Error bands correspond to one-sigma uncertainties over the ensemble
  of networks. (Right) Evolution during training of the first five eigenvalues
  of the NTK using L0 data.}
  \label{fig:NTKTime}
\end{figure}
% ===================================

It is clear from our data that the NTK does not remain constant during training.
Two different phases can be distinguished in the figure. The first one covers
the initial part of the training. From the left panel of Fig.~\ref{fig:NTKTime},
we see that the norm of the NTK varies significantly in the early stages of the
evolution, in strong contrast with the predictions obtained in the
infinite-width limit. Note also that this initial peak is more pronounced for L2
data. This is consistent with the fact that the NTK (\ie\ the architecture)
needs to accommodate the noise in the data, thus leading to a larger variation
of the NTK. On the other hand, after this initial phase -- corresponding
approximately to the first 20,000 epochs in our experiment -- the NTK tends to
stabilize. As discussed in the previous Section, we refer to this second phase
as the \textit{lazy training}, in keeping with the terminology adopted in the
literature. We conclude that, in this phase, the NTK does not change
significantly. As a consequence, this suggests that a description of the
training using a constant NTK, as predicted by the theory of the infinite-width
networks, can only be applied after the initial phase, \ie\ after the NTK has
stabilized. 

\FloatBarrier

\paragraph{Eigenvalues During Training}

Further insight on the evolution of the NTK can be obtained by studying its
eigensystem as a function of the training time. In the right panel of
Fig.~\ref{fig:NTKTime} we report the variation of the first five eigenvalues of
the NTK, using the standard NNPDF architecture and L0 data. We see that the
hierachical structure observed at initialization is preserved, but the size of
the subdominant eigenvalues increases significantly in the early stages of
training -- by one or two orders of magnitude depending on the specific
eigenvalues. 

% ===================================
\begin{figure}[t]
  \centering
  \includegraphics[width=0.30\textwidth]{figs/section_3/ntk_eigvals_L0_L1_L2_n_1.pdf}
  \includegraphics[width=0.30\textwidth]{figs/section_3/ntk_eigvals_L0_L1_L2_n_2.pdf}
  \includegraphics[width=0.30\textwidth]{figs/section_3/ntk_eigvals_L0_L1_L2_n_3.pdf}
  \includegraphics[width=0.30\textwidth]{figs/section_3/ntk_eigvals_L0_L1_L2_n_4.pdf}
  \includegraphics[width=0.30\textwidth]{figs/section_3/ntk_eigvals_L0_L1_L2_n_5.pdf}
  \caption{The first five eigenvalues of the NTK for L0, L1, and L2 data. Error
  bands correspond to one-sigma uncertainties over the ensemble of networks.}
  \label{fig:EigvalsComparison}
\end{figure}
% ===================================

In Fig.~\ref{fig:EigvalsComparison}, the same first five eigenvalues of the NTK
are displayed for L0, L1, and L2 data. We can make a few observations upon
inspecting these plots. First, we notice that the way in which data is generated
has an impact on the eigenvalues of the NTK. In general, the uncertainty bands
for L2 data are larger than those for L1 and L0 data, indicating that the NTK is
more sensitive to the noise in the data. This is consistent with the observation
made in Fig.~\ref{fig:NTKTime}. The eigenvalues reach a plateau and do not
change significantly once the network enters the lazy training regime. The
increase of the subdominant eigenvalues, combined with the analysis of
Eqs.~\eqref{eq:FlowParallel} and~\eqref{eq:FlowPerp}, suggests that more
``physical'' features become learnable before lazy training sets in.

% ===================================
\begin{figure}[t]
  \centering
  \includegraphics[width=0.45\textwidth]{section_3/loss_and_eigvals_vs_epochs.pdf}  
  \caption{Variation of the loss function overlaid with the first five
  eigenvalues for a selected replica over the ensemble. Left scale refers to the
  loss, while the right scale refers to the eigenvalues.}
  \label{fig:Loss}
\end{figure}
% ===================================

Finally, in Fig.~\ref{fig:Loss} we show the variation of the loss function
during training, overlaid with the first five eigenvalues of the NTK, for a
selected replica over the ensemble. It is interesting to see that in
correspondence with the sudden variation of the subdominant eigenvalues, the
loss function drops significantly, at the cost of an instability localised in
the descent. We interpret this as the network learning new features, changing
its internal representation to accommodate the new information. After this
initial phase, the eigenvalues stabilize and the loss function decreases
smoothly, as expected in the lazy training regime.

As it will be extensively discussed later in Sec.~\ref{sec:LazyTraining}, the
eigenvalues and eigenvectors of the NTK play a special role. Indeed, the output
$f$ can be decomposed into the basis of eigenvectors of the NTK. Hence the
eigenvectors corresponding to the larger eigenvalues can be interpreted as {\em
learnable}\ features, while the small (or zero) eigenvalues, correspond to
directions in which the field $f$ never evolves during training.

% \FloatBarrier

\subsubsection{Eigenvectors and Alignment of the NTK}
\label{sec:NTKAlign}

It has been argued before that there is a non-trivial interplay between the
eigenspace of the NTK and that of the matrix $M$. Indeed, the former encodes the
model dependence, while the latter brings physical information. Of course the
two matrices are independent at initialisation, and we do not expect any
alignment patter between the two. However, this picture does change during
training, as the NTK evolves and the model learns the target function. To
quantify this alignment, we define the matrix $A$, 
\begin{equation}
  \label{eq:MatrixA}
  A_{kk'} = \left( \left< z^{(k)}, v^{(k')}\right> \right)^2 = \cos^2(\theta_{kk'}) \;,
\end{equation}
where $z^{(k)}$ and $v^{(k')}$ are the $k$-th and $k'$-th eigenvectors of the
NTK and $M$, respectively. The matrix $A$ is thus a measure of the alignment
between the eigenspaces of the two matrices. The rows of the matrix correspond
to the eigenvectors of the NTK, ordered by the value of the corresponding
eigenvalues, with the eigenvectors corresponding to the larger eigenvalues at
the top of the matrix. The columns correspond to eigenvectors of the matrix $M$,
also ordered by the values of the corresponding eigenvalues, with the largest
eigenvalues to the left in this case. In Fig.~\ref{fig:NtkMAlign}, we show the
matrix $A$ at different epochs of the training for L2 data and a single NTK
replica. 
% ===================================
\begin{figure}[ht!]
  \centering
  \includegraphics[width=1\textwidth]{section_3/ntk_alignment_L2.pdf}
  \caption{Matrix $A$ as defined in Eq.~\eqref{eq:MatrixA} for L2 data and for a
  single replica of the NTK. The matrix is shown at different epochs of the
  training process, indicated in the top of each panel. The white dashed line
  indicates the cut-off tolerance that we imposed to the eigenvalues of the NTK
  (see Appendix...?).}
  \label{fig:NtkMAlign}
\end{figure}
% ===================================

% ===================================
\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.45\textwidth]{section_3/ntk_alignment_fin_L2_1}
  \includegraphics[width=0.45\textwidth]{section_3/ntk_alignment_fin_L2_2}
  \caption{Alignment of the eigenvectors of the NTK with the input function
  $f^{\rm(in)}$ used to generate the L2 data, measured in terms of $\cos
  \theta^{(k)} = (z^{(k)}, f^{\rm{(in)}})/ \Vert f^{\rm{(in)}}\Vert$. In the
  left panel, the first five eigenvectors of the NTK are shown, while the right
  panel shows the remaining eigenvectors up to $k=7$. \ac{Is this plot telling
  us something?}}
  \label{fig:NTKAlignFin}
\end{figure}
% ===================================
% ===================================
\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.90\textwidth]{section_3/q_directions_sign.pdf}
  \caption{First five eigenvectors of the combined matrix $H=\Theta M$, as in
  Eq.~\eqref{eq:FlowEquationNoIndices}, at different training time and as
  function of the input $x$-grid. We also show the output of the network at the
  same training time, which is displayed in gray.}
  \label{fig:NTKMEigVecs}
\end{figure}
% ===================================

The blue rectangle in the top right corner of the matrix shows that the
eigenvectors of the NTK corresponding to the largest eigenvalues are orthogonal
to the eigenvectors of $M$ that are in the kernel of $M$, \ie\ the directions
that do not contribute to the observables. It is useful to remember that the
largest eigenvalues of the NTK correspond to the directions that are orthogonal
to $\ker\Theta$, \ie\ the directions that are learnable during the training
process. In order to have a robust training process, we expect these learnable
directions to align with the directions that actually contribute to the loss
functions, \ie\ the ones corresponding to the largest eigenvalues of $M$.
Consistently with this intuition, we see that the size of this blue rectangle
increases with training time. In particular, it is clear from our plot that it
becomes deeper by the onset of the lazy training regime: more of the learnable
directions -- the {\it features}\ that the network can learn -- are aligned with
the directions that contribute most to the observables.

A similar analysis can also be performed by studying the alignment of the
eigenvectors of the NTK with the input function used to generate the data. In
Fig.~\ref{fig:NTKAlignFin}, we show the cosine of the angle between the two
vectors for two subsects of eigenvectors, in the right and left panel
respectively. In these two plots, different pattern can be observed. First, the
first four angles vary considerably during training. These observations lead us
to conclude that other than changing the dominant eigenvectors, the NTK is also
activating some others that were asleep in the first stage of training. This in
accordance to what has been previously observed in
Figs.~\ref{fig:EigvalsComparison}-\ref{fig:NtkMAlign}. 

A complementary picture is displayed in Fig.~\ref{fig:NTKMEigVecs}. Here, we
show the eigenvectors of the matrix $H = \Theta M$, labelled with $q^{(i)}$, at
different training times and as functions of the $x$-grid. Together with the
eigenvectors, we also show the output of the trained neural network at the
corresponding training time. From these plots, we see that as the training
progress, the shape of the eigenvectors become more structured in order to
reproduce the output function. Again, this conclusion supports the observations
made previously in various occasions, that during training the neural network is
changing its internal representation and the NTK encodes this information.

\FloatBarrier
