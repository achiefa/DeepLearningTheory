\documentclass[11pt]{article}

\include{init}

\newcommand{\ldd}[1]{\textcolor{red}{\textbf{Luigi: #1}}}
\newcommand{\ac}[1]{\textcolor{red}{\textbf{Amedeo: #1}}}

\title{Simplified Expressions for the Analytical Solution of the NNPDF Training Dynamics}
\author{Amedeo Chiefa}
\author{Luigi Del Debbio}
\author{Richard Kenway}
\affil{Higgs Centre for Theoretical Physics, School of Physics and Astronomy,
Peter~Guthrie~Tait~Road, Edinburgh EH9 3FD, United Kingdom.}

\date{\today}
\makeindex

\begin{document}

\maketitle

Collecting all terms yields a simple (and useful!) expression,
\begin{align}
    \label{eq:AnalyticSol}
    f_{t,\alpha}
        = U(t)_{\alpha\alpha'} f_{0,\alpha'} + V(t)_{\alpha I} Y_{I}\, .
\end{align}
The two evolution operators $U(t)$ and $V(t)$ have lengthy, yet explicit, expressions, which we
summarise here or move to an appendix: 
\begin{align}
    U(t)_{\alpha\alpha'} = \hat{U}^\perp(t)_{\alpha\alpha'}
        + \check{U}^\perp(t)_{\alpha\alpha'} + U^\parallel_{\alpha\alpha'}\, ,
\end{align}
where
\begin{align}
    \hat{U}^\perp(t)_{\alpha\alpha'}
        = \sum_{k,k'\in\perp} \sqrt{\lambda^{(k)}} z^{(k)}_\alpha 
            \left[\sum_i w^{(i)}_{k} e^{-h^{(i)}t} w^{(i)}_{k'}\right]
            z^{(k')}_{\alpha'} \sqrt{\lambda^{(k')}}\, ,
\end{align}
and
\begin{align}
    U^\parallel_{\alpha\alpha'}
        = \sum_{k''\in\parallel} z^{(k)}_\alpha z^{(k)}_{\alpha'} \, .
\end{align}
In order to write the other two contributions, we introduce the operator
\begin{align}
    \mathcal{M}_{\alpha\alpha'}(t) 
        = \sum_{k,k'\in\perp} \sqrt{\lambda^{(k)}} z^{(k)}_\alpha 
            \left[\sideset{}{'}\sum_{i} w^{(i)}_{k} \frac{1}{h^{(i)}}\, 
            \left( 1- e^{-h^{(i)}t}\right) w^{(i)}_{k'}\right]
            z^{(k')}_{\alpha'} \sqrt{\lambda^{(k')}}\,. 
\end{align}
Then, we can write
\begin{align}
    \label{eq:UperpCheck}
    \check{U}^\perp(t)
        = - \mathcal{M}\; \FKtabT C_Y^{-1} \FKtab 
            \left[\sum_{k''\in\parallel} z^{(k'')} z^{(k'') T}\right]\, ,
\end{align}
and
\begin{align}
    V(t) = \mathcal{M}\; \FKtabT C_Y^{-1}\, ,
\end{align}
where we note that the term in the bracket in Eq.~\eqref{eq:UperpCheck} is simply the projector on the 
kernel of the NTK. 

\paragraph{Physical Interpretation.} The four terms that appear in the analytical solution have a clear physical interpretation. 
\begin{itemize}
    \item The first term $\hat{U}^\perp(t)$ suppresses the components of the initial condition that lie in the subspace orthogonal 
    to the kernel of the NTK. These are the components that are learned by the network during training. The suppression 
    of the dependence on the initial condition is exponential in the training time, and the rates are given by the eigenvalues of 
    $H^{\perp}$.
    \item The contribution from $U^\parallel$ yields the component of the initial condition that lies in the kernel of the NTK. 
    As such, those components remain unchanged during training and are part of the trained field at all times $t$. 
    \item The two remaining contributions can be physically interpreted by combining them together,
    \begin{align}
        \label{eq:DataCorrectedInference}
        \check{U}^{\perp}(t) f_{0} + V(t) Y 
            = \mathcal{M}\; \FKtabT C_Y^{-1} \left[Y - \FKtab f_{0}^{\parallel}\right]\, .
    \end{align}
    The parallel component of the initial condition $f_{0}^{\parallel}$ does not evolve during training, and therefore it yields
    a contribution $\FKtab f_{0}^{\parallel}$ to the theoretical prediction of the data points at all times $t$. This is 
    taken into account by subtracting this
    contribution from the data, before the inference is performed.
\end{itemize}

\end{document}